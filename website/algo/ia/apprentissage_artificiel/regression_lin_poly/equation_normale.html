<!DOCTYPE html>
<html>
   <head>
      <meta charset="utf-8">

      <!-- CSS -->
      <link rel="stylesheet" type="text/css" href="/css/default.css">
      <link rel="stylesheet" type="text/css" href="/css/highlight_theme.css">

      <!-- Icon -->
      <link rel="icon" type="image/x-icon" href="/img/favicon.ico">

      <!-- Syntax highlighting -->
      <script src="/js/highlight.pack.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>

      <!-- Renders LaTeX expression -->
      <script type="text/x-mathjax-config">
         MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
      </script>
      <script type="text/javascript" async 
              src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
      </script>

      <title>Equation normale - napnac</title>
   </head>

   <body>
      <div id="page">
         <header>
            <a href="/">
               <img src="/img/logo.png" alt="Logo du site" height="100" width="300">
            </a>
         </header>

         <nav>
            <ul id="main_menu">
               <li><a href="/">Accueil</a></li>
               <li><a href="/articles.html">Articles</a></li>
               <li><a href="/projets.html">Projets</a></li>
               <li><a href="/a_propos.html">A propos</a></li>
            </ul>
         </nav>

         

<h1>Equation normale</h1>

<p>Publié le : 20/04/2016</p>
<p>Modifié le : 20/04/2016</p>

<h2>Introduction</h2>
<p>Contrairement à l&rsquo;<a href="/algo/ia/apprentissage_artificiel/regression_lin_poly/algo_gradient.html">algorithme du gradient</a> l&rsquo;équation normale est une expression mathématique et non pas un algorithme itératif, qui permet de résoudre notre problème de <a href="/algo/ia/apprentissage_artificiel/regression_lin_poly.html">régression linéaire</a> en trouvant les valeurs de $\theta$ pour lesquelles notre fonction $J$ est minimisée.</p>
<h2>Principe</h2>
<p>L&rsquo;équation normale est définie comme ceci :</p>
<p>$\theta = (x^\intercal x)^{-1} x^\intercal y$</p>
<p>On peut grâce à cela calculer directement $\theta$ qui permet d&rsquo;obtenir notre fonction d&rsquo;hypothèse tel que $h_{\theta} \simeq y$.</p>
<h2>Démonstration</h2>
<p>Reprenons notre fonction d&rsquo;hypothèse :</p>
<p>$h_{\theta} = x\theta$</p>
<p>Ainsi que notre fonction d&rsquo;erreur :</p>
<p>$J(\theta) = \frac{1}{2m} \displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^2$</p>
<p>Vu que $x$ et $\theta$ sont deux matrices, on peut se permettre de réécrire notre fonction $J$ en utilisant des opérations matricielles et en remplaçant la fonction d&rsquo;hypothèse par son contenu :</p>
<p>$J(\theta) = \frac{1}{2m} (x\theta - y)^\intercal (x\theta - y)$</p>
<p>Or, soit $A$ et $B$ deux matrices on a $(A + B)^\intercal = A^\intercal + B^\intercal$, donc :</p>
<p>$J(\theta) = \frac{1}{2m} ((x\theta)^\intercal - y^\intercal)(x\theta - y)$</p>
<p>Plus loin dans la démonstration on va dériver cette fonction puis la comparer à 0, le facteur $\frac{1}{2m}$ sera donc inutile et il n&rsquo;y a pas besoin de le garder ici :</p>
<p>$J(\theta) = ((x\theta)^\intercal - y^\intercal)(x\theta - y)$</p>
<p>Développons les deux facteurs :</p>
<p>$J(\theta) = (x\theta)^\intercal x\theta - (x\theta)^\intercal y - y^\intercal (x\theta) + y^\intercal y$</p>
<p>On sait que $y$ est un vecteur et que le résultat de la multiplication matricielle $x\theta$ l&rsquo;est aussi, l&rsquo;ordre de multiplication des deux ne change donc rien et on peut simplifier $- (x\theta)^\intercal y - y^\intercal (x\theta)$ en $-2(x\theta)^\intercal y$ :</p>
<p>$J(\theta) = (x\theta)^\intercal x\theta -2(x\theta)^\intercal y + y^\intercal y$</p>
<p>On peut continuer de développer notre expression puisque $(AB)^\intercal = B^\intercal A^\intercal$ :</p>
<p>$J(\theta) = \theta^\intercal x^\intercal x\theta - 2(x\theta)^\intercal y + y^\intercal y$</p>
<p>Pour trouver $\theta$ à partir de cette expression, il faut <a href="http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/">dériver</a> la fonction $J$ et comparer le résultat à 0. On obtient :</p>
<p>$\frac{\partial J}{\partial\theta} = 2x^\intercal x\theta - 2x^\intercal y$</p>
<p>$2x^\intercal x\theta - 2x^\intercal y = 0$</p>
<p>On ajoute $2x^\intercal y$ de chaque côté de l&rsquo;équation, et on divise par deux l&rsquo;expression :</p>
<p>$x^\intercal x\theta = x^\intercal y$</p>
<p>Si la matrice résultant du calcul de $x^\intercal x$ est <strong>inversible</strong>, alors on peut multiplier les deux côtés par l&rsquo;inverse de cette dernière et ainsi affirmer que :</p>
<p>$\theta = (x^\intercal x)^{-1} x^\intercal y$</p>
<p>On retrouve bien notre équation normale.</p>
<h2>Complexité</h2>
<p>La raison pour laquelle cette méthode n&rsquo;est pas tout le temps utilisée est assez simple : la <strong>rapidité</strong>.</p>
<p>En effet, le produit matriciel est encore un problème ouvert car on ne sait pas s&rsquo;il existe de meilleurs algorithmes que ceux employés aujourd&rsquo;hui. L&rsquo;algorithme naïf de multiplication matriciel a une complexité en temps de $O(N^3)$ avec $N$ le nombre de lignes des matrices, cependant, les autres algorithmes n&rsquo;ont pas une complexité si différente ($O(N^{2.807})$ pour l&rsquo;<a href="https://en.wikipedia.org/wiki/Strassen_algorithm">algorithme de Strassen</a> ou encore $O(N^{2.376})$ pour l&rsquo;<a href="https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm">algorithme de Coppersmith-Winograd</a>).</p>
<p>Il est donc peu envisageable d&rsquo;implémenter la méthode de l&rsquo;équation normale lorsqu&rsquo;on a environ $n &gt; 10000$.</p>
<h2>Implémentation</h2>
<p>Le code en Python permettant de calculer les paramètres $\theta$ avec l&rsquo;équation normale :</p>
<pre><code class="py">import numpy as np


# x = exemple d'entrée
# y = exemple de sortie
# m = nombre d'exemples
# n = nombre d'attributs
# theta = coefficients de notre fonction d'hypothese

class regression_lineaire:

    def __init__(self, entree):
        with open(entree) as f:
            self.m, self.n = map(int, f.readline().split())

        self.x = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=(list(range(self.n))), ndmin=2))
        self.y = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=([self.n]), ndmin=2))

        # Ajoute une colonne de 1 au début de notre matrice x
        col = np.ones((self.m, 1))
        self.x = np.matrix(np.hstack((col, self.x)))
        self.n = self.n + 1

    def equation_normale(self):
        x_t = np.transpose(self.x)
        self.theta = (x_t * self.x).I * x_t * self.y


ia = regression_lineaire(&quot;test01.in&quot;)
ia.equation_normale()

print(&quot;Coefficients de la fonction d'hypothese :\n&quot;)
for j in range(ia.n):
    print(&quot;theta &quot;, j, &quot; : &quot;, float(ia.theta[j]))
</code></pre>

<p>Afin d&rsquo;optimiser légèrement le programme, la matrice transposée de $x$ est stockée dans une variable car on doit la calculer deux fois (il est donc parfaitement inutile de refaire la même opération, même si ce n&rsquo;est pas l&rsquo;une des plus couteuses).</p>
<p>En entrée de notre programme, on donne le même fichier que pour l&rsquo;algorithme du gradient :</p>
<pre><code class="nohighlight">6 1
1.73 1.94
4.07 2.87
5.34 5.01
7.14 6.74
9.56 7.71
12.26 8.6
</code></pre>

<p>En sortie en revanche, on obtient des paramètres $\theta$ différents car l&rsquo;initialisation de $\theta$, le coefficient d&rsquo;apprentissage, le nombre d&rsquo;itérations maximum et l&rsquo;opération de <em>feature scaling</em> influent sur le résultat :</p>
<pre><code class="nohighlight">Coefficients de la fonction d'hypothese :

theta  0  :  0.9424111325332967
theta  1  :  0.678691601117212
</code></pre>

<p>Et voici la représentation graphique de notre fonction d&rsquo;hypothèse trouvée (le code utilisé est le même que celui pour l&rsquo;algorithme du gradient) :</p>
<figure><img alt="Sortie graphique du programme" src="/img/algo/ia/apprentissage_artificiel/regression_lin_poly/equation_normale/sortie_prog.png" /><figcaption>Sortie graphique du programme</figcaption>
</figure>
<h2>Conclusion</h2>
<p>La méthode de l&rsquo;équation normale est donc plus précise que celle de l&rsquo;algorithme du gradient car elle calcule le minimum global de la fonction d&rsquo;erreur en déterminant $\theta$ directement avec une relation mathématique. Cependant, on ne peut pas employer cette équation tout le temps car elle a une complexité en temps trop élevée, ce qui la rend quasiment inutilisable sur des entrées où $n &gt; 10000$.</p>



         <footer>
            <br>
            <hr>
            <p>Une question ? Une suggestion ? N'hésitez pas à me <a href="/a_propos.html">contacter</a> pour me communiquer vos remarques.</p>
            <br>
         </footer>
      </div>
   </body>
</html>