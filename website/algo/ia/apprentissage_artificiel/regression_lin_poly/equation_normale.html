<!DOCTYPE html>
<html>
   <head>
      <meta charset="utf-8">

      <!-- CSS -->
      <link rel="stylesheet" type="text/css" href="/css/default.css">
      <link rel="stylesheet" type="text/css" href="/css/highlight_theme.css">

      <!-- Icon -->
      <link rel="icon" type="image/x-icon" href="/img/favicon.ico">

      <!-- Syntax highlighting -->
      <script src="/js/highlight.pack.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>

      <!-- Renders LaTeX expression -->
      <script type="text/x-mathjax-config">
         MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
      </script>
      <script type="text/javascript" async 
              src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
      </script>

      <!-- Font Awesome -->
      <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

      <title>Equation normale - napnac</title>
   </head>

   <body>
      <!-- Javascript -->
      <script type="text/javascript" src="/js/utils.js"></script>

      <div id="sidebar">
         <a href="/"><img src="/img/logo.png" alt="Logo du site" height="125" width="125"></a>

         <div id="menu">
            <ul>
               <li><a href="/">Accueil</a></li>
               <li><a href="/articles.html">Articles</a></li>
               <li><a href="/projets.html">Projets</a></li>
               <li><a href="/a_propos.html">A propos</a></li>
            </ul>
         </div>

         <div id="info">
            <ul>
               <li><a href="https://twitter.com/napnac"><i class="fa fa-twitter fa-fw" aria-hidden="true"></i>Twitter</a></li>
               <li><a href="https://github.com/napnac"><i class="fa fa-github fa-fw" aria-hidden="true"></i>Github</a></li>
               <li><i class="fa fa-envelope fa-fw" aria-hidden="true"></i>napnac [at] domaine</li>
               <li><i class="fa fa-rss fa-fw" aria-hidden="true"></i><a href="https://napnac.fr/feed/rss.xml">RSS</a> / <a href="https://napnac.fr/feed/atom.xml">Atom</a></li>
            </ul>
         </div>

   <hr>
   <p>Publié le : 20/04/2016<br>Modifié le : 20/04/2016</p>

      <h4>Table des matières</h4>
      <div class="toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#principe">Principe</a></li>
<li><a href="#demonstration">Démonstration</a></li>
<li><a href="#complexite">Complexité</a></li>
<li><a href="#implementation">Implémentation</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

      </div>

      <div id="content">
         <h1><a href="">Equation normale</a></h1>

         <h2 id="introduction"><a class="toclink" href="#introduction">Introduction</a></h2>
<p>Contrairement à l'<a href="/algo/ia/apprentissage_artificiel/regression_lin_poly/algo_gradient.html">algorithme du gradient</a> l'équation normale est une expression mathématique et non pas un algorithme itératif, qui permet de résoudre notre problème de <a href="/algo/ia/apprentissage_artificiel/regression_lin_poly.html">régression linéaire</a> en trouvant les valeurs de $\theta$ pour lesquelles notre fonction $J$ est minimisée.</p>
<h2 id="principe"><a class="toclink" href="#principe">Principe</a></h2>
<p>L'équation normale est définie comme ceci :</p>
<p>$\theta = (x^\intercal x)^{-1} x^\intercal y$</p>
<p>On peut grâce à cela calculer directement $\theta$ qui permet d'obtenir notre fonction d'hypothèse tel que $h_{\theta} \simeq y$.</p>
<h2 id="demonstration"><a class="toclink" href="#demonstration">Démonstration</a></h2>
<p>Reprenons notre fonction d'hypothèse :</p>
<p>$h_{\theta} = x\theta$</p>
<p>Ainsi que notre fonction d'erreur :</p>
<p>$J(\theta) = \frac{1}{2m} \displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^2$</p>
<p>Vu que $x$ et $\theta$ sont deux matrices, on peut se permettre de réécrire notre fonction $J$ en utilisant des opérations matricielles et en remplaçant la fonction d'hypothèse par son contenu :</p>
<p>$J(\theta) = \frac{1}{2m} (x\theta - y)^\intercal (x\theta - y)$</p>
<p>Or, soit $A$ et $B$ deux matrices on a $(A + B)^\intercal = A^\intercal + B^\intercal$, donc :</p>
<p>$J(\theta) = \frac{1}{2m} ((x\theta)^\intercal - y^\intercal)(x\theta - y)$</p>
<p>Plus loin dans la démonstration on va dériver cette fonction puis la comparer à 0, le facteur $\frac{1}{2m}$ sera donc inutile et il n'y a pas besoin de le garder ici :</p>
<p>$J(\theta) = ((x\theta)^\intercal - y^\intercal)(x\theta - y)$</p>
<p>Développons les deux facteurs :</p>
<p>$J(\theta) = (x\theta)^\intercal x\theta - (x\theta)^\intercal y - y^\intercal (x\theta) + y^\intercal y$</p>
<p>On sait que $y$ est un vecteur et que le résultat de la multiplication matricielle $x\theta$ l'est aussi, l'ordre de multiplication des deux ne change donc rien et on peut simplifier $- (x\theta)^\intercal y - y^\intercal (x\theta)$ en $-2(x\theta)^\intercal y$ :</p>
<p>$J(\theta) = (x\theta)^\intercal x\theta -2(x\theta)^\intercal y + y^\intercal y$</p>
<p>On peut continuer de développer notre expression puisque $(AB)^\intercal = B^\intercal A^\intercal$ :</p>
<p>$J(\theta) = \theta^\intercal x^\intercal x\theta - 2(x\theta)^\intercal y + y^\intercal y$</p>
<p>Pour trouver $\theta$ à partir de cette expression, il faut <a href="http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/">dériver</a> la fonction $J$ et comparer le résultat à 0. On obtient :</p>
<p>$\frac{\partial J}{\partial\theta} = 2x^\intercal x\theta - 2x^\intercal y$</p>
<p>$2x^\intercal x\theta - 2x^\intercal y = 0$</p>
<p>On ajoute $2x^\intercal y$ de chaque côté de l'équation, et on divise par deux l'expression :</p>
<p>$x^\intercal x\theta = x^\intercal y$</p>
<p>Si la matrice résultant du calcul de $x^\intercal x$ est <strong>inversible</strong>, alors on peut multiplier les deux côtés par l'inverse de cette dernière et ainsi affirmer que :</p>
<p>$\theta = (x^\intercal x)^{-1} x^\intercal y$</p>
<p>On retrouve bien notre équation normale.</p>
<h2 id="complexite"><a class="toclink" href="#complexite">Complexité</a></h2>
<p>La raison pour laquelle cette méthode n'est pas tout le temps utilisée est assez simple : la <strong>rapidité</strong>.</p>
<p>En effet, le produit matriciel est encore un problème ouvert car on ne sait pas s'il existe de meilleurs algorithmes que ceux employés aujourd'hui. L'algorithme naïf de multiplication matriciel a une complexité en temps de $O(N^3)$ avec $N$ le nombre de lignes des matrices, cependant, les autres algorithmes n'ont pas une complexité si différente ($O(N^{2.807})$ pour l'<a href="https://en.wikipedia.org/wiki/Strassen_algorithm">algorithme de Strassen</a> ou encore $O(N^{2.376})$ pour l'<a href="https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm">algorithme de Coppersmith-Winograd</a>).</p>
<p>Il est donc peu envisageable d'implémenter la méthode de l'équation normale lorsqu'on a environ $n &gt; 10000$.</p>
<h2 id="implementation"><a class="toclink" href="#implementation">Implémentation</a></h2>
<p>Le code en Python permettant de calculer les paramètres $\theta$ avec l'équation normale :</p>
<pre><code class="py">import numpy as np


# x = exemple d'entrée
# y = exemple de sortie
# m = nombre d'exemples
# n = nombre d'attributs
# theta = coefficients de notre fonction d'hypothese

class regression_lineaire:

    def __init__(self, entree):
        with open(entree) as f:
            self.m, self.n = map(int, f.readline().split())

        self.x = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=(list(range(self.n))), ndmin=2))
        self.y = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=([self.n]), ndmin=2))

        # Ajoute une colonne de 1 au début de notre matrice x
        col = np.ones((self.m, 1))
        self.x = np.matrix(np.hstack((col, self.x)))
        self.n = self.n + 1

    def equation_normale(self):
        x_t = np.transpose(self.x)
        self.theta = (x_t * self.x).I * x_t * self.y


ia = regression_lineaire(&quot;test01.in&quot;)
ia.equation_normale()

print(&quot;Coefficients de la fonction d'hypothese :\n&quot;)
for j in range(ia.n):
    print(&quot;theta &quot;, j, &quot; : &quot;, float(ia.theta[j]))
</code></pre>

<p>Afin d'optimiser légèrement le programme, la matrice transposée de $x$ est stockée dans une variable car on doit la calculer deux fois (il est donc parfaitement inutile de refaire la même opération, même si ce n'est pas l'une des plus couteuses).</p>
<p>En entrée de notre programme, on donne le même fichier que pour l'algorithme du gradient :</p>
<pre><code class="nohighlight">6 1
1.73 1.94
4.07 2.87
5.34 5.01
7.14 6.74
9.56 7.71
12.26 8.6
</code></pre>

<p>En sortie en revanche, on obtient des paramètres $\theta$ différents car l'initialisation de $\theta$, le coefficient d'apprentissage, le nombre d'itérations maximum et l'opération de <em>feature scaling</em> influent sur le résultat :</p>
<pre><code class="nohighlight">Coefficients de la fonction d'hypothese :

theta  0  :  0.9424111325332967
theta  1  :  0.678691601117212
</code></pre>

<p>Et voici la représentation graphique de notre fonction d'hypothèse trouvée (le code utilisé est le même que celui pour l'algorithme du gradient) :</p>
<figure><img alt="Sortie graphique du programme" src="/img/algo/ia/apprentissage_artificiel/regression_lin_poly/equation_normale/sortie_prog.png" /><figcaption>Sortie graphique du programme</figcaption>
</figure>
<h2 id="conclusion"><a class="toclink" href="#conclusion">Conclusion</a></h2>
<p>La méthode de l'équation normale est donc plus précise que celle de l'algorithme du gradient car elle calcule le minimum global de la fonction d'erreur en déterminant $\theta$ directement avec une relation mathématique. Cependant, on ne peut pas employer cette équation tout le temps car elle a une complexité en temps trop élevée, ce qui la rend quasiment inutilisable sur des entrées où $n &gt; 10000$.</p>

         <footer>
            <br>
            <hr>
            <p>Une question ? Une suggestion ? N'hésitez pas à me <a href="/a_propos.html">contacter</a> pour me communiquer vos remarques.</p>
            <br>
         </footer>
      </div>
   </body>
</html>