<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

        <link rel="stylesheet" type="text/css" href="/css/base.css">
        <link rel="icon" type="image/x-icon" href="/img/favicon.ico">

        <!-- Font Awesome -->
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

        <!-- Syntax highlighting -->
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/highlight.min.js"></script>
        <link rel="stylesheet" type="text/css" href="/css/highlight_theme.css">
        <script>hljs.highlightAll();</script>

        <!-- Renders LaTeX expression -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true}});
        </script>
        <script type="text/javascript" async 
                src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
        </script>

        <title>Équation normale</title>

    </head>

    <body>
        <header>
    <nav>
        <ul>
            <li><a href="/">Thibault Allançon</a></li>
            <li><a href="/articles.html">Articles</a></li>
        </ul>
    </nav>

    <h1><a href="">Équation normale</a></h1>

            <p class="metadata">Publié : 20/04/2016 · Modifié : 20/04/2016</p>
        <hr>
        </header>


        <h2 id="introduction">Introduction</h2>
<p>Contrairement à l'<a
href="/algo/ia/apprentissage_artificiel/regression_lin_poly/algo_gradient.html">algorithme
du gradient</a> l'équation normale est une expression mathématique et
non pas un algorithme itératif, qui permet de résoudre notre problème de
<a
href="/algo/ia/apprentissage_artificiel/regression_lin_poly.html">régression
linéaire</a> en trouvant les valeurs de <span
class="math inline">\(\theta\)</span> pour lesquelles notre fonction
<span class="math inline">\(J\)</span> est minimisée.</p>
<h2 id="principe">Principe</h2>
<p>L'équation normale est définie comme ceci :</p>
<p><span class="math inline">\(\theta = (x^\intercal x)^{-1} x^\intercal
y\)</span></p>
<p>On peut grâce à cela calculer directement <span
class="math inline">\(\theta\)</span> qui permet d'obtenir notre
fonction d'hypothèse tel que <span class="math inline">\(h_{\theta}
\simeq y\)</span>.</p>
<h2 id="démonstration">Démonstration</h2>
<p>Reprenons notre fonction d'hypothèse :</p>
<p><span class="math inline">\(h_{\theta} = x\theta\)</span></p>
<p>Ainsi que notre fonction d'erreur :</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2m}
\displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^2\)</span></p>
<p>Vu que <span class="math inline">\(x\)</span> et <span
class="math inline">\(\theta\)</span> sont deux matrices, on peut se
permettre de réécrire notre fonction <span
class="math inline">\(J\)</span> en utilisant des opérations
matricielles et en remplaçant la fonction d'hypothèse par son contenu
:</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2m} (x\theta -
y)^\intercal (x\theta - y)\)</span></p>
<p>Or, soit <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> deux matrices on a <span
class="math inline">\((A + B)^\intercal = A^\intercal +
B^\intercal\)</span>, donc :</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2m}
((x\theta)^\intercal - y^\intercal)(x\theta - y)\)</span></p>
<p>Plus loin dans la démonstration on va dériver cette fonction puis la
comparer à 0, le facteur <span
class="math inline">\(\frac{1}{2m}\)</span> sera donc inutile et il n'y
a pas besoin de le garder ici :</p>
<p><span class="math inline">\(J(\theta) = ((x\theta)^\intercal -
y^\intercal)(x\theta - y)\)</span></p>
<p>Développons les deux facteurs :</p>
<p><span class="math inline">\(J(\theta) = (x\theta)^\intercal x\theta -
(x\theta)^\intercal y - y^\intercal (x\theta) + y^\intercal
y\)</span></p>
<p>On sait que <span class="math inline">\(y\)</span> est un vecteur et
que le résultat de la multiplication matricielle <span
class="math inline">\(x\theta\)</span> l'est aussi, l'ordre de
multiplication des deux ne change donc rien et on peut simplifier <span
class="math inline">\(- (x\theta)^\intercal y - y^\intercal
(x\theta)\)</span> en <span class="math inline">\(-2(x\theta)^\intercal
y\)</span> :</p>
<p><span class="math inline">\(J(\theta) = (x\theta)^\intercal x\theta
-2(x\theta)^\intercal y + y^\intercal y\)</span></p>
<p>On peut continuer de développer notre expression puisque <span
class="math inline">\((AB)^\intercal = B^\intercal A^\intercal\)</span>
:</p>
<p><span class="math inline">\(J(\theta) = \theta^\intercal x^\intercal
x\theta - 2(x\theta)^\intercal y + y^\intercal y\)</span></p>
<p>Pour trouver <span class="math inline">\(\theta\)</span> à partir de
cette expression, il faut <a
href="http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/">dériver</a>
la fonction <span class="math inline">\(J\)</span> et comparer le
résultat à 0. On obtient :</p>
<p><span class="math inline">\(\frac{\partial J}{\partial\theta} =
2x^\intercal x\theta - 2x^\intercal y\)</span></p>
<p><span class="math inline">\(2x^\intercal x\theta - 2x^\intercal y =
0\)</span></p>
<p>On ajoute <span class="math inline">\(2x^\intercal y\)</span> de
chaque côté de l'équation, et on divise par deux l'expression :</p>
<p><span class="math inline">\(x^\intercal x\theta = x^\intercal
y\)</span></p>
<p>Si la matrice résultant du calcul de <span
class="math inline">\(x^\intercal x\)</span> est
<strong>inversible</strong>, alors on peut multiplier les deux côtés par
l'inverse de cette dernière et ainsi affirmer que :</p>
<p><span class="math inline">\(\theta = (x^\intercal x)^{-1} x^\intercal
y\)</span></p>
<p>On retrouve bien notre équation normale.</p>
<h2 id="complexité">Complexité</h2>
<p>La raison pour laquelle cette méthode n'est pas tout le temps
utilisée est assez simple : la <strong>rapidité</strong>.</p>
<p>En effet, le produit matriciel est encore un problème ouvert car on
ne sait pas s'il existe de meilleurs algorithmes que ceux employés
aujourd'hui. L'algorithme naïf de multiplication matriciel a une
complexité en temps de <span class="math inline">\(O(N^3)\)</span> avec
<span class="math inline">\(N\)</span> le nombre de lignes des matrices,
cependant, les autres algorithmes n'ont pas une complexité si différente
(<span class="math inline">\(O(N^{2.807})\)</span> pour l'<a
href="https://en.wikipedia.org/wiki/Strassen_algorithm">algorithme de
Strassen</a> ou encore <span class="math inline">\(O(N^{2.376})\)</span>
pour l'<a
href="https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm">algorithme
de Coppersmith-Winograd</a>).</p>
<p>Il est donc peu envisageable d'implémenter la méthode de l'équation
normale lorsqu'on a environ <span class="math inline">\(n &gt;
10000\)</span>.</p>
<h2 id="implémentation">Implémentation</h2>
<p>Le code en Python permettant de calculer les paramètres <span
class="math inline">\(\theta\)</span> avec l'équation normale :</p>
<details>
<summary>equation_normale.py</summary>
<pre><code class="py">import numpy as np


# x = exemple d&#39;entrée
# y = exemple de sortie
# m = nombre d&#39;exemples
# n = nombre d&#39;attributs
# theta = coefficients de notre fonction d&#39;hypothese

class regression_lineaire:

    def __init__(self, entree):
        with open(entree) as f:
            self.m, self.n = map(int, f.readline().split())

        self.x = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=(list(range(self.n))), ndmin=2))
        self.y = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=([self.n]), ndmin=2))

        # Ajoute une colonne de 1 au début de notre matrice x
        col = np.ones((self.m, 1))
        self.x = np.matrix(np.hstack((col, self.x)))
        self.n = self.n + 1

    def equation_normale(self):
        x_t = np.transpose(self.x)
        self.theta = (x_t * self.x).I * x_t * self.y


ia = regression_lineaire(&quot;test01.in&quot;)
ia.equation_normale()

print(&quot;Coefficients de la fonction d&#39;hypothese :\n&quot;)
for j in range(ia.n):
    print(&quot;theta &quot;, j, &quot; : &quot;, float(ia.theta[j]))</code></pre>
<p>Afin d'optimiser légèrement le programme, la matrice transposée de
<span class="math inline">\(x\)</span> est stockée dans une variable car
on doit la calculer deux fois (il est donc parfaitement inutile de
refaire la même opération, même si ce n'est pas l'une des plus
couteuses).</p>
</details>
<p>En entrée de notre programme, on donne le même fichier que pour
l'algorithme du gradient :</p>
<pre><code class="nohighlight">6 1
1.73 1.94
4.07 2.87
5.34 5.01
7.14 6.74
9.56 7.71
12.26 8.6</code></pre>
<p>En sortie en revanche, on obtient des paramètres <span
class="math inline">\(\theta\)</span> différents car l'initialisation de
<span class="math inline">\(\theta\)</span>, le coefficient
d'apprentissage, le nombre d'itérations maximum et l'opération de
<em>feature scaling</em> influent sur le résultat :</p>
<pre><code class="nohighlight">Coefficients de la fonction d&#39;hypothese :

theta  0  :  0.9424111325332967
theta  1  :  0.678691601117212</code></pre>
<p>Et voici la représentation graphique de notre fonction d'hypothèse
trouvée (le code utilisé est le même que celui pour l'algorithme du
gradient) :</p>
<figure>
<img
src="/img/algo/ia/apprentissage_artificiel/regression_lin_poly/equation_normale/sortie_prog.png"
alt="/img/algo/ia/apprentissage_artificiel/regression_lin_poly/equation_normale/sortie_prog.png" />
<figcaption>Sortie graphique du programme</figcaption>
</figure>
<h2 id="conclusion">Conclusion</h2>
<p>La méthode de l'équation normale est donc plus précise que celle de
l'algorithme du gradient car elle calcule le minimum global de la
fonction d'erreur en déterminant <span
class="math inline">\(\theta\)</span> directement avec une relation
mathématique. Cependant, on ne peut pas employer cette équation tout le
temps car elle a une complexité en temps trop élevée, ce qui la rend
quasiment inutilisable sur des entrées où <span class="math inline">\(n
&gt; 10000\)</span>.</p>



        <footer>
        </footer>
    </body>
</html>