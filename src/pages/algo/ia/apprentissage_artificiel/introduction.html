<!DOCTYPE html>
<html>
   <head>
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato&#38;subset=latin,latin-ext" type="text/css" />
      <link rel="icon" type="image/x-icon" href="//static.napnac.ga/img/favicon.ico">
      <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/github-gist.min.css">

      <!-- Syntax highlighting -->
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>

      <!-- Renders LaTeX expression -->
      <script type="text/x-mathjax-config">
         MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
      </script>
      <script type="text/javascript" async
         src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
      </script>

      <!-- CSS -->
<style>
body {
   font-family: "Helvetica Neue", 'Lato', Helvetica, sans-serif;
   max-width: 1000px;
   margin: 0 auto;
   position: relative;
   width: 95%;
   line-height: 1.5;
}

/* ---- Titles ---- */

h1 {
   padding-top: 2%;
   padding-bottom: 2%;
   color: #DE4834;
}

h2, h3, h4, h5, h6 {
   padding-top: 1%;
   padding-bottom: 1%;
   color: #DE4834;
}

/* ---- Link ---- */

a {
   text-decoration: none;
   color: #2E64FE;
}

/* ---- List (+ main menu list) ---- */

ul {
   padding-left: 30px;
}

#main_menu {
   list-style: none;
   margin: 0;
   padding: 0;
   text-align: center;
}
#main_menu li {
   display: inline;
   margin-right: 1px;
}
#main_menu li a {
   line-height: 1em;
   padding: 4px 20px;
   text-align: center;
}
#main_menu li a:hover, #main_menu li a:active {
   text-decoration: underline;
}

/* ---- Tables (same look as from github markdown layout) ---- */

table {
   display: block;
   width: 100%;
   overflow: auto;
   word-break: normal;
   word-break: keep-all;
   border-collapse: collapse;
   border-spacing: 0;
   margin-top: 0;
   margin-bottom: 16px;
}

table th {
   font-weight: bold;
}

table th,
table td {
   padding: 6px 13px;
   border: 1px solid #ddd;
}

table tr {
   background-color: #fff;
   border-top: 1px solid #ccc;
}

table tr:nth-child(2n) {
   background-color: #f8f8f8;
}

/* ---- Image and caption ---- */

.figure {
   text-align: center;
}

.caption {
   font-style: italic;
   text-align: center;
}

/* ---- Summary ---- */

#summary {
   width: 70%;
   text-align: justify;
   line-height: 1.6;
}

/* ---- Code ---- */
pre {
   width: 90%;
   white-space: pre-wrap;
   word-break: break-all;
   word-wrap: break-word;
}
</style>
      <!---- ---->

      <title>Introduction Ã  l'apprentissage artificiel - napnac</title>
   </head>

   <body>

      <!-- Javascript -->
<script type="text/javascript">
function toggle_visibility(id) {
   var element = document.getElementById(id);
   if(element.style.display == 'block')
      element.style.display = 'none';
   else
      element.style.display = 'block';
}
</script>
      <!---- ---->

      <header>
         <a href="/">
            <img src="//static.napnac.ga/img/logo.png" alt="Logo du site" height="100" width="300">
         </a>

      </header>

      <nav>
         <ul id="main_menu">
            <li><a href="/">Accueil</a></li>
            <li><a href="/articles.html">Articles</a></li>
            <li><a href="/projets.html">Projets</a></li>
            <li><a href="/a_propos.html">A propos</a></li>
         </ul>
      </nav>

      <!-- Page/Article -->

<a href=""><h1 id="introduction-&#224;-lapprentissage-artificiel">Introduction &#224; l'apprentissage artificiel</h1></a>
<p>Publi&#233; le : 07/04/2016<br />
<em>Modifi&#233; le : 08/04/2016</em></p>
<ul id="summary">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#principe">Principe</a></li>
<li><a href="#type-dapprentissage">Type d'apprentissage</a></li>
<li><a href="#probl&#232;mes-&#224;-r&#233;soudre">Probl&#232;mes &#224; r&#233;soudre</a></li>
<li><a href="#domaine-dapplication">Domaine d'application</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>Si vous suivez r&#233;guli&#232;rement les nouvelles dans le domaine de l'informatique, vous &#234;tes sans doute d&#233;j&#224; tomb&#233; sur un article parlant d'un programme utilisant le principe d'apprentissage artificiel (ou <em>machine learning</em> en anglais) afin de r&#233;soudre efficacement une t&#226;che qui parait compliqu&#233;e voire impossible pour un ordinateur. Si ce n'est pas le cas, il suffit de taper &quot;machine learning&quot; dans un moteur de recherche et regarder les r&#233;sultats dans la section &quot;actualit&#233;s&quot; pour apercevoir un nombre impressionnant d'articles r&#233;cents &#224; ce propos et sur des th&#232;mes tr&#232;s vari&#233;s. Mais qu'est-ce que l'apprentissage artificiel ? Dans quel domaine peut-on l'appliquer ? Et surtout quels sont concr&#232;tement les algorithmes mis en place par ce type d'apprentissage ?</p>
<h2 id="principe">Principe</h2>
<p>En g&#233;n&#233;ral, lorsqu'on cherche &#224; r&#233;soudre un probl&#232;me avec un ordinateur, on lui demande de r&#233;aliser une suite d'op&#233;rations bien pr&#233;cise pour avoir une sortie voulue. Cependant, dans certains cas il est tr&#232;s compliqu&#233; de d&#233;finir cette suite d'op&#233;rations pour un probl&#232;me donn&#233;, m&#234;me si nous sommes capable en tant qu'humain de le r&#233;soudre sans difficult&#233;. Par exemple, essayez de d&#233;crire &#224; votre machine comment reconna&#238;tre un visage sur une photo, pour l'ordinateur l'image n'est qu'un amas de pixels, et la notion de visage ne lui &#233;voque rien du tout, cependant vous &#234;tes capable instantan&#233;ment de le faire et cela depuis tout petit. Le but de l'apprentissage artificiel n'est pas de d&#233;crire directement des op&#233;rations afin de r&#233;soudre un probl&#232;me, mais plut&#244;t de donner la capacit&#233; &#224; l'ordinateur d'apprendre &#224; r&#233;soudre ce probl&#232;me par <strong>lui-m&#234;me</strong>. Pour apprendre, il va proc&#233;der comme vous et moi lorsqu'on essaie de s'am&#233;liorer dans une discipline, c'est-&#224;-dire en essayant, en &#233;chouant, et en adaptant sa strat&#233;gie dans le but d'&#234;tre meilleur la prochaine fois.</p>
<p>L'apprentissage artificiel (ou parfois appel&#233; <em>apprentissage automatique</em>) d&#233;signe un domaine de l'intelligence artificielle visant &#224; <strong>reproduire un comportement sp&#233;cifique</strong> &#224; partir de <strong>donn&#233;es</strong>. Ce comportement est en g&#233;n&#233;ral cr&#233;&#233; de toute pi&#232;ce, et sera ensuite am&#233;lior&#233; automatiquement lors du processus d'apprentissage.</p>
<p>On peut alors comprendre &#224; quel point ce domaine est int&#233;ressant vu la complexit&#233; aujourd'hui d'automatiser certaines t&#226;ches (d'autant plus qu'on a acc&#232;s rapidement &#224; &#233;norm&#233;ment de donn&#233;es gr&#226;ce &#224; Internet, rendant l'apprentissage toujours plus efficace). Cependant, l'apprentissage artificiel n'est pas uniquement utilis&#233; pour automatiser une t&#226;che complexe qu'un humain peut faire, mais il est aussi capable d'atteindre des niveaux d'efficacit&#233; bien sup&#233;rieurs &#224; ceux d'un humain (et c'est l&#224; tout l'int&#233;r&#234;t de la discipline).</p>
<h2 id="type-dapprentissage">Type d'apprentissage</h2>
<p>Il existe de nombreuses mani&#232;res de faire apprendre quelque chose &#224; votre ordinateur, et il faudra choisir la plus adapt&#233;es en fonction du but de votre intelligence artificielle. Voici quelques exemples de diff&#233;rentes m&#233;thodes :</p>
<ul>
<li><strong>Apprentissage supervis&#233;</strong> : pour reprendre le probl&#232;me de d&#233;tection de visage, on pourrait fournir &#224; notre ordinateur des centaines (ou milliers) de photos en pr&#233;cisant nous m&#234;me o&#249; se trouve le visage. L'ordinateur va ensuite essayer de g&#233;n&#233;raliser et cr&#233;er un <strong>mod&#232;le</strong> &#224; partir de l'entr&#233;e fournit afin de pouvoir deviner par la suite o&#249; se situe un visage sur une nouvelle photo. Cet apprentissage est dit supervis&#233; car on fournit &#224; l'ordinateur des <strong>exemples</strong> de probl&#232;mes qu'il doit r&#233;soudre, mais aussi leurs <strong>solutions correspondantes</strong> afin qu'il &#233;tablisse des liens entre ces derniers.</li>
<li><strong>Apprentissage non supervis&#233;</strong> : on donne cette fois ci &#224; l'ordinateur uniquement les entr&#233;es, et on aimerait qu'il g&#233;n&#232;re et regroupe automatiquement les donn&#233;es dans diff&#233;rentes classes (qu'on appelle aussi <strong>&#233;tiquette</strong>). Par exemple, vous avez une liste d'articles de journaux que vous aimeriez trier en fonction du sujet abord&#233; dedans, l'id&#233;e serait que votre algorithme regroupe les articles discutant d'un sujet commun ensemble. L'entr&#233;e serait alors les articles, tandis que les sorties seraient les cat&#233;gories auxquelles appartiennent les diff&#233;rents articles. Ici, notre algorithme ne re&#231;oit pas les solutions des probl&#232;mes qu'on lui pose, mais seulement l'&#233;nonc&#233;, et c'est &#224; lui d'essayer de trouver une solution. Ceci peut &#234;tre tr&#232;s utile quand nous m&#234;me nous ne connaissons pas la solution, ou bien qu'on tente de trouver une nouvelle approche int&#233;ressante &#224; laquelle on n'a pas pens&#233; avant.</li>
<li><strong>Apprentissage semi-supervis&#233;</strong> : l'entr&#233;e contient un m&#233;lange des deux derni&#232;res m&#233;thodes, c'est-&#224;-dire des entr&#233;es avec des sorties correspondantes et d'autres sans. L'int&#233;r&#234;t est dans un premier temps de r&#233;duire la charge de travail sur le traitement des donn&#233;es lorsqu'on a des entr&#233;es colossales (attribuer une sortie &#224; chaque entr&#233;e peut &#234;tre long et fastidieux, voire n&#233;cessiter des moyens importants pour d&#233;cider de la sortie). De plus, cela peut am&#233;liorer radicalement l'efficacit&#233; de l'apprentissage (gr&#226;ce &#224; un entra&#238;nement plus diversifi&#233; comprenant en g&#233;n&#233;ral peu de sorties attribu&#233;es pour beaucoup d'entr&#233;es sans sorties correspondantes). Imaginez que vous &#234;tes un m&#233;decin cherchant &#224; d&#233;tecter des tumeurs, il peut &#234;tre tr&#232;s couteux de faire des analyses sur des milliers voir millions de patients pour entra&#238;ner une intelligence artificielle (sans m&#234;me prendre en compte le temps n&#233;cessaire &#224; l'op&#233;ration), et donc il serait plus pratique de fournir &#224; votre algorithme quelques &#233;tudes tr&#232;s d&#233;taill&#233;es ainsi que des analyses sans r&#233;ponses afin qu'il construise un mod&#232;le de pr&#233;diction.</li>
<li><strong>Apprentissage par renforcement</strong> : ce dernier diff&#232;re des autres dans le sens o&#249; il ne re&#231;oit pas d'entr&#233;es (au sens d'exemples d'un probl&#232;me), ni de sorties, mais plut&#244;t la description d'un <strong>environnement</strong> dans lequel il peut &#233;voluer gr&#226;ce &#224; diff&#233;rentes <strong>actions</strong> ayant un certain <strong>impact</strong> sur ses futures d&#233;cisions. Un exemple d'apprentissage par renforcement serait un <a href="https://en.wikipedia.org/wiki/Genetic_algorithm">algorithme g&#233;n&#233;tique</a>, qui consiste &#224; reproduire le comportement de la nature &#224; travers une s&#233;lection naturelle afin de faire &#233;voluer des <strong>agents</strong> charg&#233;s de fonctionner dans l'environnement choisi. Un exemple concret serait un jeu vid&#233;o, ce dernier contient un environnement d&#233;fini ainsi que plusieurs actions qui ont des r&#233;percussions sur notre agent. Il y a beaucoup d'exemples d'intelligence artificielle qui apprenne &#224; jouer &#224; un jeu vid&#233;o, comme : <a href="https://www.youtube.com/watch?v=qv6UVOQ0F44">Super Mario World</a>, <a href="https://github.com/yenchenlin1994/DeepLearningFlappyBird">Flappy Bird</a>, <a href="https://github.com/asrivat1/DeepLearningVideoGames">Pong/Tetris</a>, <a href="https://github.com/nneonneo/2048-ai">2048</a>, ou m&#234;me le <a href="https://www.youtube.com/watch?v=P7XHzqZjXQs&amp;feature=youtu.be">jeu cach&#233; int&#233;gr&#233; dans Chrome</a>, etc.</li>
</ul>
<h2 id="probl&#232;mes-&#224;-r&#233;soudre">Probl&#232;mes &#224; r&#233;soudre</h2>
<p>Avec ces diff&#233;rents types d'apprentissages apparait plusieurs algorithmes, mais avant de comprendre comment fonctionnent ces algorithmes, cherchons &#224; savoir ce qu'ils essaient de r&#233;soudre. Il y a plusieurs probl&#232;mes importants dans le domaine de l'apprentissage artificiel, en voici une liste non exhaustive :</p>
<ul>
<li><strong>Classification</strong> : on cherche &#224; attribuer plusieurs entr&#233;es une <strong>classe</strong> bien pr&#233;cise (dans un apprentissage supervis&#233; donc, car notre algorithme aura re&#231;u un entra&#238;nement compos&#233; d'entr&#233;es et de leurs classes correspondantes), ces classes peuvent &#234;tre par exemple dans le cadre d'une description d'une image : &quot;animal&quot;, &quot;paysage&quot;, &quot;montagne&quot;, &quot;for&#234;t&quot;, etc.</li>
<li><strong>R&#233;gression</strong> : les valeurs obtenues dans un probl&#232;me de r&#233;gression sont des <strong>valeurs num&#233;riques</strong>, par exemple l'estimation du prix d'un appartement en fonction de diff&#233;rents crit&#232;res comme sa taille, son emplacement g&#233;ographique, le nombre de pi&#232;ces, son anciennet&#233;, etc. L&#224; aussi, ce probl&#232;me se pose dans un apprentissage supervis&#233;.</li>
<li><strong>Partitionnement</strong> (ou <em>clustering</em> en anglais) : on veut que l'algorithme s&#233;pare les entr&#233;es en diff&#233;rents groupes distincts (appel&#233;s <em>cluster</em> en anglais), mais c'est &#224; l'algorithme de d&#233;cider des groupes &#224; attribuer aux entr&#233;es. Ceci est donc un probl&#232;me dans le domaine de l'apprentissage non supervis&#233;.</li>
</ul>
<p>L'apprentissage artificiel utilise les math&#233;matiques afin de r&#233;soudre ces probl&#232;mes, par exemple un exercice de classification pourrait &#234;tre repr&#233;sent&#233; graphiquement par la s&#233;paration efficace (gr&#226;ce &#224; une ligne, courbe, etc.) de deux ensembles distincts :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/intro/exemple_classification.png" alt="Exemple de classification" />
<p class="caption">Exemple de classification</p>
</div>
<p>Les deux param&#232;tres de l'entr&#233;e sont l'axe x et l'axe y, et les sorties sont les classes correspondantes (rond ou croix).</p>
<p>Dans un probl&#232;me de r&#233;gression, on cherchera par exemple &#224; g&#233;n&#233;raliser l'entr&#233;e (l'axe x dans notre cas) &#224; l'aide d'une fonction, ici elle est lin&#233;aire, mais on peut trouver des fonctions polynomiales complexes pour cr&#233;er des mod&#232;les plus sophistiqu&#233;s :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/intro/exemple_regression.png" alt="Exemple de r&#233;gression" />
<p class="caption">Exemple de r&#233;gression</p>
</div>
<p>Ce mod&#232;le nous permettra de g&#233;n&#233;rer une sortie (l'axe y) sur de nouveaux param&#232;tres, en utilisant la fonction calcul&#233;e (ici elle sera donc de la forme <span class="math inline">\(y = ax + b\)</span>).</p>
<p>Enfin, un exemple de partitionnement serait de regrouper des points entre eux, et de les diff&#233;rencier en groupe, comme ici avec trois diff&#233;rents clusters (rouge, bleu, et vert) :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/intro/exemple_partitionnement.png" alt="Exemple de partitionnement" />
<p class="caption">Exemple de partitionnement</p>
</div>
<h2 id="domaine-dapplication">Domaine d'application</h2>
<p>L'apprentissage artificiel est pr&#233;sent partout :</p>
<ul>
<li><strong>M&#233;decine</strong> : afin de faire des diagnostics, un ordinateur peut &#234;tre un atout majeur car il a acc&#232;s &#224; des milliers d'exemples de patients et peut ainsi effectuer un diagnostic automatiquement et relativement pr&#233;cis sur le type de maladie, son avancement, etc. (ex : <a href="https://en.wikipedia.org/wiki/Watson_%28computer%29">Watson</a> une intelligence artificielle d&#233;velopp&#233;e par IBM qui a des utilisations multiples, et notamment dans la m&#233;decine).</li>
<li><strong>Moteur de recherche</strong> : la plupart des moteurs de recherches modernes utilisent des algorithmes de machine learning autant pour comprendre la requ&#234;te de l'utilisateur, que pour la chercher ou encore trier les r&#233;sultats en fonction de la pertinence.</li>
<li><strong>Communication</strong> : la reconnaissance vocale, ou la compr&#233;hension d'un message &#233;crit sont des probl&#232;mes extr&#234;mement complexes &#224; r&#233;soudre, et pourtant gr&#226;ce &#224; l'apprentissage artificiel, on arrive parfois &#224; des r&#233;sultats assez impressionnant de pr&#233;cision hors normes. <a href="http://www.soundhound.com/hound">Hound</a> avait fait beaucoup de bruit en 2015 gr&#226;ce &#224; une <a href="https://www.youtube.com/watch?v=M1ONXea0mXg">vid&#233;o</a> qui d&#233;montrait une efficacit&#233; incroyable compar&#233;e &#224; ses autres concurrents (Siri, Google Now, Cortana, etc.).</li>
<li><strong>Traitement d'images</strong> : que ce soit pour reconna&#238;tre un visage, ou d&#233;crire une image, l'apprentissage artificiel est souvent indispensable vu la complexit&#233; de la t&#226;che. Facebook a par exemple un algorithme terriblement efficace de reconnaissance de visage (<a href="https://research.facebook.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/">DeepFace</a>), gr&#226;ce au nombre colossal de photos envoy&#233;es sur la plateforme en ligne (servant alors d'exemple et d'entra&#238;nement &#224; l'algorithme). Ce dernier est tellement puissant qu'il est m&#234;me capable de vous reconna&#238;tre lorsque vous ne regardez pas la cam&#233;ra, quand vous avez le visage partiellement couvert, ou bien encore quand vous &#234;tes de dos. Google a aussi d&#233;velopp&#233; un <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">programme</a> capable de faire une description tr&#232;s pr&#233;cise d'une image ce qui est assez bluffant pour un ordinateur vu la complexit&#233; de certaines images.</li>
<li><strong>Internet</strong> : la plupart des r&#233;seaux sociaux (Facebook, Twitter, ...), des sites de ventes (Amazon, Ebay, ...), de divertissement (Netflix, Deezer, Spotify, ...) utilisent des algorithmes d'apprentissage artificiel pour vous faire des recommandations, des suggestions, afin de mieux comprendre vos go&#251;ts (qui est une notion trop abstraite pour &#234;tre enti&#232;rement d&#233;crite et pr&#233;cis&#233;e &#224; un ordinateur, d'o&#249; l'utilisation d'une intelligence artificielle).</li>
<li><strong>Transport</strong> : la fameuse <a href="https://en.wikipedia.org/wiki/Google_self-driving_car">Google Car</a> est une voiture totalement autonome et sans conducteur d&#233;velopp&#233;e par Google, qui a d&#233;pass&#233; le stade de test et roule aujourd'hui sur les routes de Californie. C'est impressionnant de voir &#224; quel point on peut aller loin avec la technologie de l'apprentissage artificiel.</li>
<li><strong>S&#233;curit&#233;</strong> : pour v&#233;rifier l'identit&#233; de quelqu'un, d'une transaction bancaire ou de l'authenticit&#233; d'un mail, les algorithmes de machine learning sont tr&#232;s utiles dans la s&#233;curit&#233; et permettent d'identifier rapidement les fraudes &#233;ventuelles. Tous les bons clients mails sont capables de filtrer automatiquement les &quot;spams&quot; et tous syst&#232;mes bancaires est aussi &#233;quip&#233;s de ce genre de protection afin d'&#233;viter tous &#233;changes douteux voire illicites.</li>
</ul>
<p>La liste des exemples de tous les jours pourrait continuer <strong>longtemps</strong>, mais on retrouve aussi cette forme d'intelligence artificielle dans des applications plus surprenantes et tr&#232;s int&#233;ressantes :</p>
<ul>
<li><strong>Art</strong> : est-ce que vous pensez savoir diff&#233;rencier une peinture faite par un homme de celle r&#233;alis&#233;e par un ordinateur ? Faites-le <a href="http://turing.deepart.io/">test</a>, vous pourriez &#234;tre surpris de voir &#224; quel point c'est difficile. Les ordinateurs savent assez bien imiter des artistes, que ce soit dans la peinture (avec l'exemple pr&#233;c&#232;dent, ou encore <a href="https://www.theguardian.com/technology/2015/sep/02/computer-algorithm-recreates-van-gogh-painting-picasso">celui-ci</a>), dans la musique (<a href="http://artsites.ucsc.edu/faculty/cope/experiments.htm">EMI</a>), dans la litt&#233;rature (<a href="http://singularityhub.com/2012/12/13/patented-book-writing-system-lets-one-professor-create-hundreds-of-thousands-of-amazon-books-and-counting/">&#233;criture de livres automatique</a>), etc. Un ordinateur peut donc imiter avec une pr&#233;cision incroyable un ph&#233;nom&#232;ne s'il est fourni assez de donn&#233;es.</li>
<li><strong>Jeu</strong> : depuis qu'un ordinateur a battu le champion du monde d'&#233;chec en <a href="https://en.wikipedia.org/wiki/Deep_Blue_%28chess_computer%29">1997</a>, la comp&#233;tition entre humain et ordinateur est rude. Le <a href="https://en.wikipedia.org/wiki/Go_%28game%29">jeu de Go</a> &#233;tait l'un des derniers jeux classiques &#224; r&#233;sister &#224; cause de son nombre quasi infini de possibilit&#233; (contrairement au jeu d'&#233;chec, o&#249; Deep Blue explorait la plupart des possibilit&#233;s et choisissait simplement la &quot;meilleure&quot;), cependant en mars 2016, <a href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a> une intelligence artificielle d&#233;velopp&#233;e par Google a vaincu le champion du monde gr&#226;ce &#224; plusieurs algorithmes d'apprentissage artificielle ainsi qu'un entra&#238;nement intensif de plusieurs ann&#233;es de recherche. Cette prouesse technique a surpris bon nombre de sp&#233;cialistes dans le domaine, qui ne pensaient tout simplement pas atteindre un tel niveau d'intelligence en 2016, et esp&#233;raient que le jeu de Go r&#233;sisterait plus longtemps &#224; la machine.</li>
<li><strong>Pr&#233;diction</strong> : conna&#238;tre le pass&#233; est un moyen int&#233;ressant de pr&#233;voir le futur, et le super-ordinateur <a href="http://journals.uic.edu/ojs/index.php/fm/article/view/3663/3040">Nautilus</a> a amass&#233; des centaines de millions de diff&#233;rents articles datant des 30 derni&#232;res ann&#233;es ce qui lui a permis en 2011 de pr&#233;voir avec une pr&#233;cision incroyable l'arriv&#233;e du printemps Arabe, ainsi que l'endroit o&#249; se cachait Osama Bin Laden.</li>
</ul>
<p>Encore une fois, la liste peut continuer car les exemples ne manquent pas.</p>
<h2 id="conclusion">Conclusion</h2>
<p>L'apprentissage artificiel est donc un domaine extr&#234;mement vaste de l'intelligence artificielle, et qui est encore en plein d&#233;veloppement aujourd'hui. Ce dernier est particuli&#232;rement efficace lorsqu'on cherche &#224; r&#233;soudre un probl&#232;me difficile &#224; exprimer concr&#232;tement, et qu'on est capable de regrouper des donn&#233;es assez importantes dessus, afin qu'un ordinateur puisse apprendre par lui-m&#234;me comment r&#233;soudre ce probl&#232;me (et parfois m&#234;me bien plus efficacement qu'un humain). Avec des donn&#233;es de plus en plus importantes et riches en informations, cette discipline ne fait que s'am&#233;liorer d'ann&#233;es en ann&#233;es, et son utilisation ne fait que s'&#233;largir et se diversifier.</p>
<p>Cependant, cette forme d'intelligence ne sait pas tout faire et n&#233;cessite toujours l'aide d'un humain afin de l'am&#233;liorer, de lui fournir des donn&#233;es pertinentes, et l'id&#233;e d'une intelligence artificielle autonome capable d'apprendre, communiquer et prendre des d&#233;cisions importantes en effraie plus d'un. Ceci a pouss&#233; &#224; la cr&#233;ation d'une <a href="http://futureoflife.org/">organisation</a> tr&#232;s particuli&#232;re dans le but d'&#233;viter des risques majeurs d&#251;s &#224; des intelligences artificielles, et cette derni&#232;re est support&#233;e par plusieurs personnalit&#233;s importantes comme <a href="https://en.wikipedia.org/wiki/Stephen_Hawking">Stephen Hawking</a>, ou encore <a href="https://en.wikipedia.org/wiki/Elon_Musk">Elon Musk</a>. M&#234;me si l'apprentissage artificiel, voire l'intelligence artificielle de mani&#232;re g&#233;n&#233;rale, peut nous apporter &#233;norm&#233;ment d'am&#233;liorations et d'innovations impensables dans la vie de tous les jours, ceci peut tout &#224; fait apporter de grands risques et une <a href="http://futureoflife.org/open-letter-autonomous-weapons/">lettre ouverte</a> de la fondation <em>Futur of Life</em> d&#233;nonce l'automatisation d'armes de guerre qui marquent selon eux &quot;une troisi&#232;me r&#233;volution dans l'armement apr&#232;s la poudre &#224; canon et le nucl&#233;aire&quot;. Les Etats-Unis notamment utilisent d&#233;j&#224; des intelligences artificielles afin de remplacer les humains dans des programmes de drones, car les pilotes &#233;taient souvent <a href="http://www.democracynow.org/2013/10/25/a_drone_warriors_torment_ex_air">affect&#233;s mentalement</a> et d&#233;veloppaient des traumatismes les for&#231;ant &#224; arr&#234;ter.</p>
<p>L'avenir de cette discipline est cependant loin d'&#234;tre sombre, et beaucoup de projets ambitieux sont lanc&#233;s avec des objectifs tous plus fous les uns que les autres, et on commence &#224; faire des <a href="https://fr.sputniknews.com/insolite/201504251015815685/">pr&#233;dictions</a> sur le futur assez impressionnantes :</p>
<blockquote>
<p>2020 &#8211; Les ordinateurs personnels atteindront une puissance de traitement comparable au cerveau humain.</p>
</blockquote>
<blockquote>
<p>2022 &#8211; Les USA et l'Europe adopteront des lois r&#233;glementant les relations entre les individus et les robots. L'activit&#233; des robots, leurs droits, devoirs et autres restrictions seront formalis&#233;s.</p>
</blockquote>
<blockquote>
<p>2024 &#8211; Les &#233;l&#233;ments d'intelligence informatique seront obligatoires dans les voitures. Il sera interdit aux individus de conduire une voiture qui ne sera pas &#233;quip&#233;e d'une assistance informatique.</p>
</blockquote>
<blockquote>
<p>2027 &#8211; Un robot personnel capable d'accomplir des actions complexes en toute autonomie sera aussi anodin qu'un r&#233;frig&#233;rateur ou une machine &#224; caf&#233;.</p>
</blockquote>
<blockquote>
<p>2029 &#8211; L'ordinateur pourra passer le test de Turing pour prouver son intelligence dans le sens humain du terme, gr&#226;ce &#224; la simulation informatique du cerveau humain.</p>
</blockquote>
<blockquote>
<p>2033 &#8211; Les voitures sans conducteur circuleront sur les routes.</p>
</blockquote>
<blockquote>
<p>2034 &#8211; Le premier rendez-vous de l'homme avec l'intelligence artificielle. Le film Her en version plus moderne: la compagne virtuelle pourrait &#234;tre &#233;quip&#233;e d'un &quot;corps&quot; en projetant une image dans la r&#233;tine de l'&#339;il &#8211; par exemple, &#224; l'aide de lentilles ou de lunettes virtuelles.</p>
</blockquote>
<blockquote>
<p>2037 &#8211; Un progr&#232;s gigantesque sera enregistr&#233; dans la compr&#233;hension du secret du cerveau humain. Des centaines de sous-r&#233;gions ayant des fonctions sp&#233;cifiques seront d&#233;couvertes. Certains algorithmes qui codent le d&#233;veloppement de ces r&#233;gions seront d&#233;crypt&#233;s et int&#233;gr&#233;s aux r&#233;seaux neuronaux d'ordinateurs.</p>
</blockquote>
<blockquote>
<p>2038 &#8211; L'apparition de personnes robotis&#233;es et de produits de technologies transhumanistes. Ils seront dot&#233;s d'une intelligence suppl&#233;mentaire (par exemple, orient&#233;e sur une sph&#232;re concr&#232;te de connaissances que le cerveau humain est incapable de couvrir enti&#232;rement) et de divers implants optionnels &#8211; des yeux-cam&#233;ras aux bras-proth&#232;ses suppl&#233;mentaires.</p>
</blockquote>
<blockquote>
<p>2044 &#8211; L'intelligence non-biologique sera des milliards de fois plus intelligente que son homologue biologique.</p>
</blockquote>
<blockquote>
<p>2045 &#8211; Arriv&#233;e de la singularit&#233; technologique. La Terre se transformera en ordinateur gigantesque.</p>
</blockquote>
<blockquote>
<p>2099 &#8211; Le processus de singularit&#233; technologique s'&#233;tend sur tout l'Univers.</p>
</blockquote>
<p>Vu l'omni-pr&#233;sence de ce domaine aujourd'hui, et dans le futur, il est important pour un programmeur d'en comprendre la base ainsi que les principaux algorithmes afin de r&#233;soudre des probl&#232;mes d'apprentissage artificiel.</p>
 
      <!-- ------------ -->

      <footer>
         <hr>
         <p>Une question ? Une suggestion ? N'h&#233;sitez pas &#224; me <a href="/a_propos.html">contacter</a> pour me communiquer vos remarques.
         <br>
      </footer>

   </body>
</html>
