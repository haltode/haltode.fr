<!DOCTYPE html>
<html>
   <head>
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato&#38;subset=latin,latin-ext" type="text/css" />
      <link rel="icon" type="image/x-icon" href="//static.napnac.ga/img/favicon.ico">
      <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/github-gist.min.css">

      <!-- Syntax highlighting -->
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>

      <!-- Renders LaTeX expression -->
      <script type="text/x-mathjax-config">
         MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
      </script>
      <script type="text/javascript" async
         src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
      </script>

      <!-- CSS -->
<style>
body {
   font-family: "Helvetica Neue", 'Lato', Helvetica, sans-serif;
   max-width: 1000px;
   margin: 0 auto;
   position: relative;
   width: 95%;
   line-height: 1.5;
}

/* ---- Titles ---- */

h1 {
   padding-top: 2%;
   padding-bottom: 2%;
   color: #DE4834;
}

h2, h3, h4, h5, h6 {
   padding-top: 1%;
   padding-bottom: 1%;
   color: #DE4834;
}

/* ---- Link ---- */

a {
   text-decoration: none;
   color: #2E64FE;
}

/* ---- List (+ main menu list) ---- */

ul {
   padding-left: 30px;
}

#main_menu {
   list-style: none;
   margin: 0;
   padding: 0;
   text-align: center;
}
#main_menu li {
   display: inline;
   margin-right: 1px;
}
#main_menu li a {
   line-height: 1em;
   padding: 4px 20px;
   text-align: center;
}
#main_menu li a:hover, #main_menu li a:active {
   text-decoration: underline;
}

/* ---- Tables (same look as from github markdown layout) ---- */

table {
   display: block;
   width: 100%;
   overflow: auto;
   word-break: normal;
   word-break: keep-all;
   border-collapse: collapse;
   border-spacing: 0;
   margin-top: 0;
   margin-bottom: 16px;
}

table th {
   font-weight: bold;
}

table th,
table td {
   padding: 6px 13px;
   border: 1px solid #ddd;
}

table tr {
   background-color: #fff;
   border-top: 1px solid #ccc;
}

table tr:nth-child(2n) {
   background-color: #f8f8f8;
}

/* ---- Image and caption ---- */

.figure {
   text-align: center;
}

.caption {
   font-style: italic;
   text-align: center;
}

/* ---- Summary ---- */

#summary {
   width: 70%;
   text-align: justify;
   line-height: 1.6;
}

/* ---- Code ---- */
pre {
   width: 90%;
   white-space: pre-wrap;
   word-break: break-all;
   word-wrap: break-word;
}
</style>
      <!---- ---->

      <title>Algorithme du gradient - napnac</title>
   </head>

   <body>

      <!-- Javascript -->
<script type="text/javascript">
function toggle_visibility(id) {
   var element = document.getElementById(id);
   if(element.style.display == 'block')
      element.style.display = 'none';
   else
      element.style.display = 'block';
}
</script>
      <!---- ---->

      <header>
         <a href="/">
            <img src="//static.napnac.ga/img/logo.png" alt="Logo du site" height="100" width="300">
         </a>

      </header>

      <nav>
         <ul id="main_menu">
            <li><a href="/">Accueil</a></li>
            <li><a href="/articles.html">Articles</a></li>
            <li><a href="/projets.html">Projets</a></li>
            <li><a href="/a_propos.html">A propos</a></li>
         </ul>
      </nav>

      <!-- Page/Article -->

<a href=""><h1 id="algorithme-du-gradient">Algorithme du gradient</h1></a>
<p>Publi&#233; le : 14/04/2016<br />
<em>Modifi&#233; le : 14/04/2016</em></p>
<ul id="summary">
<li><a href="#principe">Principe</a></li>
<li><a href="#pseudo-code">Pseudo-code</a></li>
<li><a href="#impl&#233;mentation">Impl&#233;mentation</a></li>
</ul>
<h2 id="principe">Principe</h2>
<p>L'id&#233;e de l'algorithme est de commencer avec des param&#232;tres initiaux <span class="math inline">\(\theta\)</span> (en g&#233;n&#233;ral on utilise 0), puis d'adapter ces derniers avec le r&#233;sultat obtenu par notre fonction d'erreur, afin de minimiser <span class="math inline">\(J\)</span> pour arriver on l'esp&#232;re &#224; un minimum global.</p>
<p>Il est plus difficile de visualiser l'id&#233;e de l'algorithme sur notre pr&#233;c&#233;dent graphique en 3D, alors on va utiliser un graphique 2D sp&#233;cial qui trace les contours (on appelle cela un <a href="http://www.itl.nist.gov/div898/handbook/eda/section3/contour.htm"><em>contour plot</em></a> en anglais) :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lineaire/contour_plot.png" alt="Contour plot de notre graphique" />
<p class="caption">Contour plot de notre graphique</p>
</div>
<p>Les contours repr&#233;sentent <span class="math inline">\(J\)</span> en fonction de nos deux coefficients <span class="math inline">\(\theta_{0}\)</span> et <span class="math inline">\(\theta_{1}\)</span>. La croix rouge correspond au minimum de la fonction d'erreur, et c'est le point qu'on cherche &#224; atteindre.</p>
<p>L'algorithme du gradient va proc&#233;der ainsi :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lineaire/exemple_algo_gradient.png" alt="Exemple du fonctionnement de l&#39;algorithme du gradient" />
<p class="caption">Exemple du fonctionnement de l'algorithme du gradient</p>
</div>
<p>On part d'un point initial sur le graphique, et on fait des pas de plus en plus petits afin de se rapprocher du minimum de la fonction. Cependant, comment l'algorithme r&#233;alise-t-il ces pas ? Comment est-ce qu'il d&#233;cide de l'amplitude, ou encore de la direction &#224; emprunter ?</p>
<p>Pour comprendre l'algorithme, on peut imaginer que ce dernier utilise la &quot;pente&quot; de la repr&#233;sentation de la fonction pour d&#233;cider du prochain point &#224; explorer. Math&#233;matiquement parlant, cette d&#233;cision se fera gr&#226;ce &#224; la <a href="https://en.wikipedia.org/wiki/Partial_derivative"><strong>d&#233;riv&#233;e partielle</strong></a> de la fonction <span class="math inline">\(J\)</span> au point actuel de notre algorithme.</p>
<p>Simplifions notre probl&#232;me avec un exemple de fonction <span class="math inline">\(J\)</span> prenant uniquement un param&#232;tre <span class="math inline">\(\theta_{0}\)</span> :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lineaire/exemple_simplifie_algo_gradient.png" alt="Exemple simplifi&#233; de l&#39;algorithme du gradient" />
<p class="caption">Exemple simplifi&#233; de l'algorithme du gradient</p>
</div>
<p>On initialise l'algorithme avec un point tel que <span class="math inline">\(\theta_{0} = 0\)</span>, et on calcule la d&#233;riv&#233;e partielle de la fonction <span class="math inline">\(J\)</span> en ce point :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lineaire/exemple_simplifie_algo_gradient_init.png" alt="Initialisation" />
<p class="caption">Initialisation</p>
</div>
<p>La d&#233;riv&#233;e partielle est la droite en bleue, et on remarque que son coefficient directeur est n&#233;gatif et important, notre algorithme va donc augmenter <span class="math inline">\(\theta_{0}\)</span> de mani&#232;re importante.</p>
<p>On peut continuer ainsi jusqu'&#224; tomber sur le minimum de notre fonction :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lineaire/exemple_simplifie_algo_gradient_reste.png" alt="Reste de l&#39;algorithme" />
<p class="caption">Reste de l'algorithme</p>
</div>
<h2 id="pseudo-code">Pseudo-code</h2>
<p>Maintenant qu'on a vu le principe, il faut le d&#233;crire de mani&#232;re concr&#232;te et math&#233;matique.</p>
<p>Tant que l'algorithme ne converge pas, on met &#224; jour tous nos coefficients <span class="math inline">\(\theta\)</span> pour <span class="math inline">\(j\)</span> allant de 0 &#224; <span class="math inline">\(n\)</span> :</p>
<p><span class="math inline">\(\theta_{j} = \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta)\)</span></p>
<p><span class="math inline">\(\alpha\)</span> est notre <strong>vitesse d'apprentissage</strong>, qui sert &#224; r&#233;guler la rapidit&#233; de la convergence et la d&#233;riv&#233;e partielle de <span class="math inline">\(J\)</span> est repr&#233;sent&#233;e par <span class="math inline">\(\frac{\partial}{\partial\theta_{j}}J(\theta)\)</span>. Lorsqu'on <a href="https://math.stackexchange.com/questions/70728/partial-derivative-in-gradient-descent-for-two-variables/189792#189792">calcule cette d&#233;riv&#233;e partielle</a> on obtient :</p>
<p><span class="math inline">\(\frac{\partial}{\partial\theta_{j}}J(\theta) = \frac{1}{m}\displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})x_{ij}\)</span></p>
<p>Notre formule finale est donc :</p>
<p><span class="math inline">\(\theta_{j} = \theta_{j} - \alpha\frac{1}{m}\displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})x_{ij}\)</span></p>
<p>Il est primordial de bien choisir le coefficient d'apprentissage, car si <span class="math inline">\(\alpha\)</span> est trop &#233;lev&#233; notre algorithme va chercher &#224; faire de tr&#232;s grands pas afin de converger rapidement, ce qui peut l'amener &#224; faire de mauvais choix comme par exemple :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lineaire/exemple_coeff_apprentissage_eleve.png" alt="Exemple de cons&#233;quence d&#39;un coefficient d&#39;apprentissage &#233;lev&#233;" />
<p class="caption">Exemple de cons&#233;quence d'un coefficient d'apprentissage &#233;lev&#233;</p>
</div>
<p>De m&#234;me, une vitesse d'apprentissage trop faible rendra notre algorithme terriblement lent :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lineaire/exemple_coeff_apprentissage_faible.png" alt="Exemple de cons&#233;quence d&#39;un coefficient d&#39;apprentissage faible" />
<p class="caption">Exemple de cons&#233;quence d'un coefficient d'apprentissage faible</p>
</div>
<p>Pour choisir une valeur adapt&#233;e &#224; notre probl&#232;me, il faut en essayer diff&#233;rentes (0.001, 0.01, 0.1, 1, 10, etc.) tout en cr&#233;ant un graphique repr&#233;sentant l'&#233;volution de notre minimisation de <span class="math inline">\(J\)</span> en fonction du nombre d'it&#233;rations de l'algorithme. Si vous avez bien choisi le coefficient, vous devriez voir un graphique semblable &#224; ceci :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lineaire/exemple_coeff_apprentissage_bon.png" alt="Exemple de coefficient adapt&#233;" />
<p class="caption">Exemple de coefficient adapt&#233;</p>
</div>
<p>Dans notre d&#233;tail de l'algorithme du gradient, il y a un point tr&#232;s important &#224; ne pas n&#233;gliger : la mise &#224; jour de mani&#232;re <strong>instantan&#233;e</strong>. Vu que notre expression d&#233;pend elle-m&#234;me de <span class="math inline">\(\theta\)</span>, on ne peut pas se permettre de modifier certaines valeurs lorsqu'on met &#224; jour nos coefficients, il faut donc proc&#233;der en deux &#233;tapes bien distinctes :</p>
<ul>
<li>Mettre &#224; jour nos valeurs en utilisant des variables temporaires.</li>
<li>Copier le contenu de ces variables temporaires dans nos coefficients.</li>
</ul>
<p>Si l'on garde notre exemple avec deux attributs, on aurait ces op&#233;rations &#224; effectuer :</p>
<p><span class="math inline">\(temp0 = \theta_{0} - \alpha\frac{\partial}{\partial\theta_{0}}J(\theta_{0}, \theta_{1})\)</span></p>
<p><span class="math inline">\(temp1 = \theta_{1} - \alpha\frac{\partial}{\partial\theta_{1}}J(\theta_{0}, \theta_{1})\)</span></p>
<p><span class="math inline">\(\theta_{0} = temp0\)</span></p>
<p><span class="math inline">\(\theta_{1} = temp1\)</span></p>
<p>Le pseudo-code d&#233;finitif ressemble donc &#224; ceci :</p>
<pre class="nohighlight"><code>Tant que l&#39;algorithme ne converge pas ET qu&#39;on n&#39;a pas d&#233;pass&#233; la limite de tours
   Pour chaque coefficient
      Calculer temp[j]
   Pour chaque coefficient
      theta[j] = temp[j]</code></pre>
<p>En g&#233;n&#233;ral, on se contente de garder la deuxi&#232;me condition dans notre boucle principale afin de pouvoir fixer un nombre de tours maximum ou minimum &#224; notre algorithme (si ce dernier prend du temps par exemple, ou si on souhaite &#233;tudier sa progression).</p>
<h2 id="impl&#233;mentation">Impl&#233;mentation</h2>
<p>Par convention et pour simplifier le code, il n'est pas rare de rajouter un attribut <span class="math inline">\(x_0 = 1\)</span>, afin de remplacer notre fonction d'hypoth&#232;se <span class="math inline">\(h_{\theta}(x) = \theta_{0} + \theta_{1}x_1 + \theta_{2}x_2 + \ldots + \theta_{n}x_n\)</span>, en :</p>
<p><span class="math inline">\(h_{\theta}(x) = \displaystyle\sum_{i=0}^{n} \theta_{i}x_{i}\)</span></p>
<p>Cette op&#233;ration est ensuite r&#233;alis&#233;e tr&#232;s simplement dans notre code, car <span class="math inline">\(\theta\)</span> est une matrice contenant une seule colonne, ce qui signifie que pour calculer le r&#233;sultat de notre fonction d'hypoth&#232;se, il suffit de faire un produit matriciel.</p>
<p>Voici donc le code en Python pour l'algorithme du gradient :</p>
<p><em>J'utilise Python afin d'avoir acc&#232;s &#224; des librairies scientifiques comme <a href="http://www.numpy.org/">numpy</a> pour les matrices et <a href="http://matplotlib.org/">matplotlib</a> pour les sorties graphiques</em></p>
<a href="javascript:toggle_visibility('regression_lineaire.py');">regression_lineaire.py</a>
<div id="regression_lineaire.py" style="display: none;">
<pre class="py"><code>import numpy as np


# x = exemple d&#39;entr&#233;e
# y = exemple de sortie
# m = nombre d&#39;exemples
# n = nombre d&#39;attributs
# theta = coefficients de notre fonction d&#39;hypothese

class regression_lineaire:

    def __init__(self, entree):
        with open(entree) as f:
            self.m, self.n = map(int, f.readline().split())

        self.x = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=(list(range(self.n))), ndmin=2))
        self.y = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=([self.n]), ndmin=2))

        # Ajoute une colonne de 1 au d&#233;but de notre matrice x
        col = np.ones((self.m, 1))
        self.x = np.matrix(np.hstack((col, self.x)))
        self.n = self.n + 1

        # Initialise &#224; 0 les coefficients de la fonction d&#39;hypothese
        self.theta = np.matrix(np.zeros((self.n, 1)))

    def algo_gradient(self, alpha, nb_tour_max):
        for _ in range(nb_tour_max):
            # Pour faire la mise &#224; jour instantan&#233;e des coefficients :
            # 1. On calcule d&#39;abord les r&#233;sultats dans des variables temporaires
            temp = np.zeros((self.n, 1))
            for j in range(self.n):
                somme = 0.0
                for i in range(self.m):
                    hypothese = float(self.x[i] * self.theta)
                    somme = somme + ((hypothese - self.y[i]) * self.x.item((i, j)))
                temp[j] = self.theta[j] - alpha * (1 / self.m) * somme

            # 2. Puis on copie les r&#233;sultats dans nos coefficients
            for j in range(self.n):
                self.theta[j] = temp[j]


ia = regression_lineaire(&quot;test01.in&quot;)
ia.algo_gradient(0.01, 400)

print(&quot;Coefficients de la fonction d&#39;hypothese :\n&quot;)
for j in range(ia.n):
    print(&quot;theta &quot;, j, &quot; : &quot;, float(ia.theta[j]))</code></pre>
</div>
<p>Notre fichier d'entr&#233;e contient sur la premi&#232;re ligne le nombre <span class="math inline">\(m\)</span> d'exemples, puis le nombre <span class="math inline">\(n\)</span> d'attributs et sur les <span class="math inline">\(m\)</span> prochaines lignes une liste de nombre dont la derni&#232;re colonne correspond &#224; <span class="math inline">\(y\)</span> et les autres &#224; <span class="math inline">\(x\)</span>. J'ai repris notre exemple de l'introduction pour construire le fichier d'entr&#233;e (les unit&#233;s sont toujours en centaine d'op&#233;rations et en centaine d'euros) :</p>
<pre class="nohighlight"><code>6 1
1.73 1.94
4.07 2.87
5.34 5.01
7.14 6.74
9.56 7.71
12.26 8.6</code></pre>
<p>En sortie on obtient les coefficients <span class="math inline">\(\theta\)</span> de notre fonction d'hypoth&#232;se :</p>
<pre class="nohighlight"><code>Coefficients de la fonction d&#39;hypothese :

theta  0  :  0.5764647547614207
theta  1  :  0.7219164912370313</code></pre>
<p>Vu qu'on a uniquement un attribut, on peut repr&#233;senter notre fonction d'hypoth&#232;se et nos donn&#233;es en entr&#233;e sur un graphique 2D :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lineaire/sortie_prog_algo_gradient.png" alt="Sortie graphique du programme" />
<p class="caption">Sortie graphique du programme</p>
</div>
<p>On obtient bien une g&#233;n&#233;ralisation efficace sous forme de fonction lin&#233;aire qui ressemble fortement &#224; celle qu'un humain peut faire &#224; la main (m&#234;me si celle que l'ordinateur a calcul&#233; est plus pr&#233;cise que celle faite &#224; la main).</p>
<p>Le code utilis&#233; pour r&#233;aliser cette sortie :</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt

<span class="co"># R&#233;cup&#232;re dans des listes les valeurs de x, y, et de notre approximation de y</span>
x <span class="op">=</span> np.array(ia.x[:,<span class="dv">1</span>]).tolist()
x <span class="op">=</span> [<span class="bu">float</span>(i[<span class="dv">0</span>]) <span class="cf">for</span> i <span class="op">in</span> x]

y <span class="op">=</span> np.array(ia.y).tolist()
y <span class="op">=</span> [<span class="bu">float</span>(i[<span class="dv">0</span>]) <span class="cf">for</span> i <span class="op">in</span> y]

y_approx <span class="op">=</span> np.array(ia.x <span class="op">*</span> ia.theta).tolist()
y_approx <span class="op">=</span> [<span class="bu">float</span>(i[<span class="dv">0</span>]) <span class="cf">for</span> i <span class="op">in</span> y_approx]

<span class="co"># Affiche les points donn&#233;s en entr&#233;e, ainsi que notre mod&#232;le lin&#233;aire</span>
plt.plot(x, y, <span class="st">&#39;+&#39;</span>)
plt.plot(x, y_approx, <span class="st">&#39;r-&#39;</span>)
plt.show()</code></pre></div>
<h3 id="am&#233;liorations">Am&#233;liorations</h3>
<p>Une des premi&#232;res am&#233;liorations qu'on peut apporter &#224; notre code est celle qu'on a apport&#233; &#224; notre fonction d'hypoth&#232;se : l'utilisation des op&#233;rations des matrices. Au lieu d'appliquer des op&#233;rations sur les &#233;l&#233;ments d'une matrice un par un, on peut utiliser des op&#233;rations plus g&#233;n&#233;rales sur notre matrice enti&#232;re. Cela permet de supprimer la plupart des boucles, mais aussi &#224; l'avantage de r&#233;aliser une mise &#224; jour instantan&#233;e des coefficients automatiquement, sans m&#234;me avoir besoin de stocker nos r&#233;sultats dans des variables temporaires. On peut donc transformer notre algorithme du gradient en ceci :</p>
<p><span class="math inline">\(\theta = \theta - \alpha\frac{1}{m}x^\intercal(h_{\theta}(x) - y)\)</span></p>
<p>Si on d&#233;veloppe notre fonction d'hypoth&#232;se on arrive &#224; cette expression :</p>
<p><span class="math inline">\(\theta = \theta - \alpha\frac{1}{m}x^\intercal(x\theta - y)\)</span></p>
<p>Il n'y a plus aucunes boucles, et uniquement des op&#233;rations matricielles. Notre fonction pour l'algorithme du gradient devient donc dans notre code :</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> algo_gradient(<span class="va">self</span>, alpha, nb_tour_max):
   <span class="cf">for</span> _ <span class="op">in</span> <span class="bu">range</span>(nb_tour_max):
      derivee <span class="op">=</span> np.transpose(<span class="va">self</span>.x) <span class="op">*</span> (<span class="va">self</span>.x <span class="op">*</span> <span class="va">self</span>.theta <span class="op">-</span> <span class="va">self</span>.y)
      <span class="va">self</span>.theta <span class="op">=</span> <span class="va">self</span>.theta <span class="op">-</span> alpha <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> <span class="va">self</span>.m) <span class="op">*</span> derivee</code></pre></div>
<p>Une deuxi&#232;me am&#233;lioration concernant cette fois-ci l'algorithme en lui m&#234;me plut&#244;t que le code.</p>
<ul>
<li>feature scaling</li>
<li>mean normalization</li>
</ul>
 
      <!-- ------------ -->

      <footer>
         <hr>
         <p>Une question ? Une suggestion ? N'h&#233;sitez pas &#224; me <a href="/a_propos.html">contacter</a> pour me communiquer vos remarques.
         <br>
      </footer>

   </body>
</html>
