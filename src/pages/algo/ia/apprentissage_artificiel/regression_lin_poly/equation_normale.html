<!DOCTYPE html>
<html>
   <head>
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato&#38;subset=latin,latin-ext" type="text/css" />
      <link rel="icon" type="image/x-icon" href="//static.napnac.ga/img/favicon.ico">
      <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/github-gist.min.css">

      <!-- Syntax highlighting -->
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>

      <!-- Renders LaTeX expression with KaTeX -->
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
      <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

      <!-- CSS -->
<style>
body {
   font-family: "Helvetica Neue", 'Lato', Helvetica, sans-serif;
   max-width: 1000px;
   margin: 0 auto;
   position: relative;
   width: 95%;
   line-height: 1.5;
}

/* ---- Titles ---- */

h1 {
   padding-top: 2%;
   padding-bottom: 2%;
   color: #DE4834;
}

h2, h3, h4, h5, h6 {
   padding-top: 1%;
   padding-bottom: 1%;
   color: #DE4834;
}

/* ---- Link ---- */

a {
   text-decoration: none;
   color: #2E64FE;
}

/* ---- List (+ main menu list) ---- */

ul {
   padding-left: 30px;
}

#main_menu {
   list-style: none;
   margin: 0;
   padding: 0;
   text-align: center;
}
#main_menu li {
   display: inline;
   margin-right: 1px;
}
#main_menu li a {
   line-height: 1em;
   padding: 4px 20px;
   text-align: center;
}
#main_menu li a:hover, #main_menu li a:active {
   text-decoration: underline;
}

/* ---- Tables (same look as from github markdown layout) ---- */

table {
   display: block;
   width: 100%;
   overflow: auto;
   word-break: normal;
   word-break: keep-all;
   border-collapse: collapse;
   border-spacing: 0;
   margin-top: 0;
   margin-bottom: 16px;
}

table th {
   font-weight: bold;
}

table th,
table td {
   padding: 6px 13px;
   border: 1px solid #ddd;
}

table tr {
   background-color: #fff;
   border-top: 1px solid #ccc;
}

table tr:nth-child(2n) {
   background-color: #f8f8f8;
}

/* ---- Image and caption ---- */

.figure {
   text-align: center;
}

.caption {
   font-style: italic;
   text-align: center;
}

/* ---- Summary ---- */

#summary {
   width: 70%;
   text-align: justify;
   line-height: 1.6;
}

/* ---- Code ---- */
pre {
   width: 90%;
   white-space: pre-wrap;
   word-break: break-all;
   word-wrap: break-word;
}
</style>
      <!---- ---->

      <title>Equation normale - napnac</title>
   </head>

   <body>

      <!-- Javascript -->
<script type="text/javascript">
function toggle_visibility(id) {
   var element = document.getElementById(id);
   if(element.style.display == 'block')
      element.style.display = 'none';
   else
      element.style.display = 'block';
}
</script>
      <!---- ---->

      <header>
         <a href="/">
            <img src="//static.napnac.ga/img/logo.png" alt="Logo du site" height="100" width="300">
         </a>

      </header>

      <nav>
         <ul id="main_menu">
            <li><a href="/">Accueil</a></li>
            <li><a href="/articles.html">Articles</a></li>
            <li><a href="/projets.html">Projets</a></li>
            <li><a href="/a_propos.html">A propos</a></li>
         </ul>
      </nav>

      <!-- Page/Article -->

<a href=""><h1 id="equation-normale">Equation normale</h1></a>
<p>Publi&#233; le : 20/04/2016<br />
<em>Modifi&#233; le : 20/04/2016</em></p>
<ul id="summary">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#principe">Principe</a></li>
<li><a href="#d&#233;monstration">D&#233;monstration</a></li>
<li><a href="#complexit&#233;">Complexit&#233;</a></li>
<li><a href="#impl&#233;mentation">Impl&#233;mentation</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>Contrairement &#224; l'<a href="/algo/ia/apprentissage_artificiel/regression_lin_poly/algo_gradient.html">algorithme du gradient</a> l'&#233;quation normale est une expression math&#233;matique et non pas un algorithme it&#233;ratif, qui permet de r&#233;soudre notre probl&#232;me de <a href="/algo/ia/apprentissage_artificiel/regression_lin_poly.html">r&#233;gression lin&#233;aire</a> en trouvant les valeurs de <span class="math inline">\(\theta\)</span> pour lesquelles notre fonction <span class="math inline">\(J\)</span> est minimis&#233;e.</p>
<h2 id="principe">Principe</h2>
<p>L'&#233;quation normale est d&#233;finie comme ceci :</p>
<p><span class="math inline">\(\theta = (x^\intercal x)^{-1} x^\intercal y\)</span></p>
<p>On peut gr&#226;ce &#224; cela calculer directement <span class="math inline">\(\theta\)</span> qui permet d'obtenir notre fonction d'hypoth&#232;se tel que <span class="math inline">\(h_{\theta} \simeq y\)</span>.</p>
<h2 id="d&#233;monstration">D&#233;monstration</h2>
<p>Reprenons notre fonction d'hypoth&#232;se :</p>
<p><span class="math inline">\(h_{\theta} = x\theta\)</span></p>
<p>Ainsi que notre fonction d'erreur :</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2m} \displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^2\)</span></p>
<p>Vu que <span class="math inline">\(x\)</span> et <span class="math inline">\(\theta\)</span> sont deux matrices, on peut se permettre de r&#233;&#233;crire notre fonction <span class="math inline">\(J\)</span> en utilisant des op&#233;rations matricielles et en rempla&#231;ant la fonction d'hypoth&#232;se par son contenu :</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2m} (x\theta - y)^\intercal (x\theta - y)\)</span></p>
<p>Or, soit <span class="math inline">\(A\)</span> et <span class="math inline">\(B\)</span> deux matrices on a <span class="math inline">\((A + B)^\intercal = A^\intercal + B^\intercal\)</span>, donc :</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2m} ((x\theta)^\intercal - y^\intercal)(x\theta - y)\)</span></p>
<p>Plus loin dans la d&#233;monstration on va d&#233;river cette fonction puis la comparer &#224; 0, le facteur <span class="math inline">\(\frac{1}{2m}\)</span> sera donc inutile et il n'y a pas besoin de le garder ici :</p>
<p><span class="math inline">\(J(\theta) = ((x\theta)^\intercal - y^\intercal)(x\theta - y)\)</span></p>
<p>D&#233;veloppons les deux facteurs :</p>
<p><span class="math inline">\(J(\theta) = (x\theta)^\intercal x\theta - (x\theta)^\intercal y - y^\intercal (x\theta) + y^\intercal y\)</span></p>
<p>On sait que <span class="math inline">\(y\)</span> est un vecteur et que le r&#233;sultat de la multiplication matricielle <span class="math inline">\(x\theta\)</span> l'est aussi, l'ordre de multiplication des deux ne change donc rien et on peut simplifier <span class="math inline">\(- (x\theta)^\intercal y - y^\intercal (x\theta)\)</span> en <span class="math inline">\(-2(x\theta)^\intercal y\)</span> :</p>
<p><span class="math inline">\(J(\theta) = (x\theta)^\intercal x\theta -2(x\theta)^\intercal y + y^\intercal y\)</span></p>
<p>On peut continuer de d&#233;velopper notre expression puisque <span class="math inline">\((AB)^\intercal = B^\intercal A^\intercal\)</span> :</p>
<p><span class="math inline">\(J(\theta) = \theta^\intercal x^\intercal x\theta - 2(x\theta)^\intercal y + y^\intercal y\)</span></p>
<p>Pour trouver <span class="math inline">\(\theta\)</span> &#224; partir de cette expression, il faut <a href="http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/">d&#233;river</a> la fonction <span class="math inline">\(J\)</span> et comparer le r&#233;sultat &#224; 0. On obtient :</p>
<p><span class="math inline">\(\frac{\partial J}{\partial\theta} = 2x^\intercal x\theta - 2x^\intercal y\)</span></p>
<p><span class="math inline">\(2x^\intercal x\theta - 2x^\intercal y = 0\)</span></p>
<p>On ajoute <span class="math inline">\(2x^\intercal y\)</span> de chaque c&#244;t&#233; de l'&#233;quation, et on divise par deux l'expression :</p>
<p><span class="math inline">\(x^\intercal x\theta = x^\intercal y\)</span></p>
<p>Si la matrice r&#233;sultant du calcul de <span class="math inline">\(x^\intercal x\)</span> est <strong>inversible</strong>, alors on peut multiplier les deux c&#244;t&#233;s par l'inverse de cette derni&#232;re et ainsi affirmer que :</p>
<p><span class="math inline">\(\theta = (x^\intercal x)^{-1} x^\intercal y\)</span></p>
<p>On retrouve bien notre &#233;quation normale.</p>
<h2 id="complexit&#233;">Complexit&#233;</h2>
<p>La raison pour laquelle cette m&#233;thode n'est pas tout le temps utilis&#233;e est assez simple : la <strong>rapidit&#233;</strong>.</p>
<p>En effet, le produit matriciel est encore un probl&#232;me ouvert car on ne sait pas s'il existe de meilleurs algorithmes que ceux employ&#233;s aujourd'hui. L'algorithme na&#239;f de multiplication matriciel a une complexit&#233; en temps de <span class="math inline">\(O(N^3)\)</span> avec <span class="math inline">\(N\)</span> le nombre de lignes des matrices, cependant, les autres algorithmes n'ont pas une complexit&#233; si diff&#233;rente (<span class="math inline">\(O(N^{2.807})\)</span> pour l'<a href="https://en.wikipedia.org/wiki/Strassen_algorithm">algorithme de Strassen</a> ou encore <span class="math inline">\(O(N^{2.376})\)</span> pour l'<a href="https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm">algorithme de Coppersmith-Winograd</a>).</p>
<p>Il est donc peu envisageable d'impl&#233;menter la m&#233;thode de l'&#233;quation normale lorsqu'on a environ <span class="math inline">\(n &gt; 10000\)</span>.</p>
<h2 id="impl&#233;mentation">Impl&#233;mentation</h2>
<p>Le code en Python permettant de calculer les param&#232;tres <span class="math inline">\(\theta\)</span> avec l'&#233;quation normale :</p>
<a href="javascript:toggle_visibility('equation_normale.py');">equation_normale.py</a>
<div id="equation_normale.py" style="display: none;">
<pre class="py"><code>import numpy as np


# x = exemple d&#39;entr&#233;e
# y = exemple de sortie
# m = nombre d&#39;exemples
# n = nombre d&#39;attributs
# theta = coefficients de notre fonction d&#39;hypothese

class regression_lineaire:

    def __init__(self, entree):
        with open(entree) as f:
            self.m, self.n = map(int, f.readline().split())

        self.x = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=(list(range(self.n))), ndmin=2))
        self.y = np.matrix(np.loadtxt(entree, skiprows=1,
                            usecols=([self.n]), ndmin=2))

        # Ajoute une colonne de 1 au d&#233;but de notre matrice x
        col = np.ones((self.m, 1))
        self.x = np.matrix(np.hstack((col, self.x)))
        self.n = self.n + 1

    def equation_normale(self):
        x_t = np.transpose(self.x)
        self.theta = (x_t * self.x).I * x_t * self.y


ia = regression_lineaire(&quot;test01.in&quot;)
ia.equation_normale()

print(&quot;Coefficients de la fonction d&#39;hypothese :\n&quot;)
for j in range(ia.n):
    print(&quot;theta &quot;, j, &quot; : &quot;, float(ia.theta[j]))</code></pre>
</div>
<p>Afin d'optimiser l&#233;g&#232;rement le programme, la matrice transpos&#233;e de <span class="math inline">\(x\)</span> est stock&#233;e dans une variable car on doit la calculer deux fois (il est donc parfaitement inutile de refaire la m&#234;me op&#233;ration, m&#234;me si ce n'est pas l'une des plus couteuses).</p>
<p>En entr&#233;e de notre programme, on donne le m&#234;me fichier que pour l'algorithme du gradient :</p>
<pre class="nohighlight"><code>6 1
1.73 1.94
4.07 2.87
5.34 5.01
7.14 6.74
9.56 7.71
12.26 8.6</code></pre>
<p>En sortie en revanche, on obtient des param&#232;tres <span class="math inline">\(\theta\)</span> diff&#233;rents car l'initialisation de <span class="math inline">\(\theta\)</span>, le coefficient d'apprentissage, le nombre d'it&#233;rations maximum et l'op&#233;ration de <em>feature scaling</em> influent sur le r&#233;sultat :</p>
<pre class="nohighlight"><code>Coefficients de la fonction d&#39;hypothese :

theta  0  :  0.9424111325332967
theta  1  :  0.678691601117212</code></pre>
<p>Et voici la repr&#233;sentation graphique de notre fonction d'hypoth&#232;se trouv&#233;e (le code utilis&#233; est le m&#234;me que celui pour l'algorithme du gradient) :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/equation_normale/sortie_prog.png" alt="Sortie graphique du programme" />
<p class="caption">Sortie graphique du programme</p>
</div>
<h2 id="conclusion">Conclusion</h2>
<p>La m&#233;thode de l'&#233;quation normale est donc plus pr&#233;cise que celle de l'algorithme du gradient car elle calcule le minimum global de la fonction d'erreur en d&#233;terminant <span class="math inline">\(\theta\)</span> directement avec une relation math&#233;matique. Cependant, on ne peut pas employer cette &#233;quation tout le temps car elle a une complexit&#233; en temps trop &#233;lev&#233;e, ce qui la rend quasiment inutilisable sur des entr&#233;es o&#249; <span class="math inline">\(n &gt; 10000\)</span>.</p>
 
      <!-- ------------ -->

      <footer>
         <br>
         <hr>
         <p>Une question ? Une suggestion ? N'h&#233;sitez pas &#224; me <a href="/a_propos.html">contacter</a> pour me communiquer vos remarques.
         <br>
      </footer>

      <!-- Automatically render all of the math inside the page with KaTeX -->
      <script>
         renderMathInElement(document.body);
      </script>

   </body>
</html>
