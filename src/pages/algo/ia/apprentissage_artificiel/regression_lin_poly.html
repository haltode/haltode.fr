<!DOCTYPE html>
<html>
   <head>
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato&#38;subset=latin,latin-ext" type="text/css" />
      <link rel="icon" type="image/x-icon" href="//static.napnac.ga/img/favicon.ico">
      <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/github-gist.min.css">

      <!-- Syntax highlighting -->
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>

      <!-- Renders LaTeX expression with KaTeX -->
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
      <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

      <!-- CSS -->
<style>
body {
   font-family: "Helvetica Neue", 'Lato', Helvetica, sans-serif;
   max-width: 1000px;
   margin: 0 auto;
   position: relative;
   width: 95%;
   line-height: 1.5;
}

/* ---- Titles ---- */

h1 {
   padding-top: 2%;
   padding-bottom: 2%;
   color: #DE4834;
}

h2, h3, h4, h5, h6 {
   padding-top: 1%;
   padding-bottom: 1%;
   color: #DE4834;
}

/* ---- Link ---- */

a {
   text-decoration: none;
   color: #2E64FE;
}

/* ---- List (+ main menu list) ---- */

ul {
   padding-left: 30px;
}

#main_menu {
   list-style: none;
   margin: 0;
   padding: 0;
   text-align: center;
}
#main_menu li {
   display: inline;
   margin-right: 1px;
}
#main_menu li a {
   line-height: 1em;
   padding: 4px 20px;
   text-align: center;
}
#main_menu li a:hover, #main_menu li a:active {
   text-decoration: underline;
}

/* ---- Tables (same look as from github markdown layout) ---- */

table {
   display: block;
   width: 100%;
   overflow: auto;
   word-break: normal;
   word-break: keep-all;
   border-collapse: collapse;
   border-spacing: 0;
   margin-top: 0;
   margin-bottom: 16px;
}

table th {
   font-weight: bold;
}

table th,
table td {
   padding: 6px 13px;
   border: 1px solid #ddd;
}

table tr {
   background-color: #fff;
   border-top: 1px solid #ccc;
}

table tr:nth-child(2n) {
   background-color: #f8f8f8;
}

/* ---- Image and caption ---- */

.figure {
   text-align: center;
}

.caption {
   font-style: italic;
   text-align: center;
}

/* ---- Summary ---- */

#summary {
   width: 70%;
   text-align: justify;
   line-height: 1.6;
}

/* ---- Code ---- */
pre {
   width: 90%;
   white-space: pre-wrap;
   word-break: break-all;
   word-wrap: break-word;
}
</style>
      <!---- ---->

      <title>Régression linéaire/polynomiale - napnac</title>
   </head>

   <body>

      <!-- Javascript -->
<script type="text/javascript">
function toggle_visibility(id) {
   var element = document.getElementById(id);
   if(element.style.display == 'block')
      element.style.display = 'none';
   else
      element.style.display = 'block';
}
</script>
      <!---- ---->

      <header>
         <a href="/">
            <img src="//static.napnac.ga/img/logo.png" alt="Logo du site" height="100" width="300">
         </a>

      </header>

      <nav>
         <ul id="main_menu">
            <li><a href="/">Accueil</a></li>
            <li><a href="/articles.html">Articles</a></li>
            <li><a href="/projets.html">Projets</a></li>
            <li><a href="/a_propos.html">A propos</a></li>
         </ul>
      </nav>

      <!-- Page/Article -->

<a href=""><h1 id="r&#233;gression-lin&#233;airepolynomiale">R&#233;gression lin&#233;aire/polynomiale</h1></a>
<p>Publi&#233; le : 25/04/2016<br />
<em>Modifi&#233; le : 25/04/2016</em></p>
<ul id="summary">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#principe">Principe</a></li>
<li><a href="#fonction-derreur">Fonction d'erreur</a></li>
<li><a href="#algorithmes">Algorithmes</a></li>
<li><a href="#r&#233;gression-polynomiale">R&#233;gression polynomiale</a></li>
<li><a href="#probl&#232;mes">Probl&#232;mes</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>Vous souhaitez estimer le prix d'un ordinateur en fonction de diff&#233;rents facteurs (puissance, m&#233;moire, stockage, batterie, etc.), cependant la t&#226;che se complique au fur et &#224; mesure que vous rajoutez des possibilit&#233;s, et vous d&#233;cidez alors d'employer un algorithme d'apprentissage artificiel pour faire le travail &#224; votre place. Afin de prendre un exemple simple, on va dire que vous estimez le prix d'un ordinateur uniquement en fonction de sa puissance de calcul. Dans l'<a href="/algo/ia/apprentissage_artificiel/introduction.html">introduction &#224; la mati&#232;re</a>, on a vu que r&#233;colter des donn&#233;es utiles est une &#233;tape importante dans un processus d'apprentissage, et vous avez alors not&#233; la puissance et le prix de diff&#233;rents ordinateurs dans un tableau :</p>
<p><em>Les donn&#233;es sont totalement invent&#233;es et ne servent que d'exemple.</em></p>
<table>
<thead>
<tr class="header">
<th align="center">Puissance (nombre d'op&#233;rations/s)</th>
<th align="center">Prix (&#8364;)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">173</td>
<td align="center">194</td>
</tr>
<tr class="even">
<td align="center">407</td>
<td align="center">287</td>
</tr>
<tr class="odd">
<td align="center">534</td>
<td align="center">501</td>
</tr>
<tr class="even">
<td align="center">714</td>
<td align="center">674</td>
</tr>
<tr class="odd">
<td align="center">956</td>
<td align="center">771</td>
</tr>
<tr class="even">
<td align="center">1226</td>
<td align="center">860</td>
</tr>
</tbody>
</table>
<p>On peut repr&#233;senter ce tableau gr&#226;ce &#224; un graphique en deux dimensions tr&#232;s simple :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/exemple_donnees.png" alt="Exemple de donn&#233;es r&#233;colt&#233;es" />
<p class="caption">Exemple de donn&#233;es r&#233;colt&#233;es</p>
</div>
<p>Ce qu'on cherche &#224; faire dans notre probl&#232;me c'est d'<strong>extrapoler</strong>, c'est-&#224;-dire g&#233;n&#233;raliser gr&#226;ce aux donn&#233;es obtenues afin de pr&#233;dire un r&#233;sultat. En tant qu'humain, on pourrait facilement faire une bonne g&#233;n&#233;ralisation comme ceci :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/exemple_generalisation.png" alt="Exemple de g&#233;n&#233;ralisation" />
<p class="caption">Exemple de g&#233;n&#233;ralisation</p>
</div>
<p>On a une fonction lin&#233;aire basique, qu'on peut ensuite utiliser graphiquement pour trouver &#224; partir de la puissance d'un ordinateur, une bonne estimation de son prix.</p>
<p>Cependant, du point de vue d'une personne, il est facile de trouver un lien entre les donn&#233;es dans le but de g&#233;n&#233;raliser, mais comment un algorithme peut-il reproduire ce comportement ? Qu'est-ce qui fait qu'une fonction (lin&#233;aire, polynomiale, etc.) est une bonne g&#233;n&#233;ralisation de notre probl&#232;me ?</p>
<p>Avant de se lancer dans la recherche d'un algorithme d'apprentissage artificiel, il faut d&#233;finir ses caract&#233;ristiques. Dans notre probl&#232;me, on a un ensemble de donn&#233;es sous la forme d'entr&#233;es et de sorties correspondantes, nous sommes donc dans un <strong>apprentissage supervis&#233;</strong>. De plus, la sortie qu'on cherche est une valeur num&#233;rique, notre probl&#232;me appartient alors au domaine de la <strong>r&#233;gression</strong>. Une fois qu'on conna&#238;t ces deux informations essentielles, on peut d&#233;cider de l'algorithme &#224; utiliser en fonction de nos besoins et de nos ressources. Dans notre situation on souhaiterait faire une g&#233;n&#233;ralisation sous la forme d'une fonction lin&#233;aire, la m&#233;thode &#224; employer est donc : la <strong>r&#233;gression lin&#233;aire</strong>.</p>
<p><em>La r&#233;gression lin&#233;aire n'est qu'un cas particulier de la r&#233;gression polynomiale, mais il est plus simple de commencer avec une simple fonction lin&#233;aire pour ensuite aborder des fonctions plus complexes (m&#234;me si le principe reste exactement le m&#234;me).</em></p>
<h2 id="principe">Principe</h2>
<p>La r&#233;gression lin&#233;aire est un moyen de g&#233;n&#233;raliser et de cr&#233;er un mod&#232;le lin&#233;aire &#224; partir d'exemples. Lesdits exemples sont repr&#233;sent&#233;s &#224; l'aide de <a href="https://en.wikipedia.org/wiki/Matrix_%28mathematics%29">matrices</a> o&#249; <span class="math inline">\(x\)</span> est l'entr&#233;e, et <span class="math inline">\(y\)</span> est la sortie correspondante. Prenons le cas o&#249; on essaie de g&#233;n&#233;raliser le prix d'un appartement en fonction de sa taille et de son nombre de pi&#232;ces (encore une fois les donn&#233;es sont fictives):</p>
<table>
<thead>
<tr class="header">
<th align="center">Taille (m&#178;)</th>
<th align="center">Nombre de pi&#232;ces</th>
<th align="center">Prix (millier &#8364;)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">112</td>
<td align="center">4</td>
<td align="center">253</td>
</tr>
<tr class="even">
<td align="center">203</td>
<td align="center">6</td>
<td align="center">760</td>
</tr>
<tr class="odd">
<td align="center">158</td>
<td align="center">5</td>
<td align="center">558</td>
</tr>
<tr class="even">
<td align="center">98</td>
<td align="center">3</td>
<td align="center">243</td>
</tr>
<tr class="odd">
<td align="center">143</td>
<td align="center">4</td>
<td align="center">302</td>
</tr>
</tbody>
</table>
<p>Nos matrices ressembleront &#224; ceci :</p>
<p><span class="math inline">\(x = \begin{bmatrix} 112 &amp; 4 \\ 203 &amp; 6 \\ 158 &amp; 5 \\ 98 &amp; 3 \\ 143 &amp; 4 \end{bmatrix}\)</span> <span class="math inline">\(y = \begin{bmatrix} 253 \\ 760 \\ 558 \\ 243 \\ 302 \end{bmatrix}\)</span></p>
<p>Le <strong>mod&#232;le</strong> qu'on cherche &#224; construire est une fonction lin&#233;aire, qu'on notera :</p>
<p><span class="math inline">\(h_{\theta}(x) = \theta_{0} + \theta_{1}x_1 + \theta_{2}x_2 + \ldots + \theta_{n}x_n\)</span></p>
<p><span class="math inline">\(\theta\)</span> correspond aux coefficients de notre fonction qui prend <span class="math inline">\(n\)</span> <strong>attributs</strong> (ou <em>feature</em> en anglais). Les attributs sont les diff&#233;rentes colonnes de <span class="math inline">\(x\)</span> (la puissance d'un ordinateur, la taille d'un appartement, le nombre de pi&#232;ces, etc.) et c'est sur quoi se base l'algorithme pour g&#233;n&#233;raliser le probl&#232;me qu'on lui donne. Il est tr&#232;s important de bien choisir les attributs, et de renseigner uniquement ceux qui ont une r&#233;elle influence sur le r&#233;sultat, mais sans pour autant en donner trop peu &#224; l'algorithme.</p>
<p>On souhaite donc trouver une fonction <span class="math inline">\(h_{\theta}\)</span>, aussi appel&#233;e <strong>fonction d'hypoth&#232;se</strong>, tel que <span class="math inline">\(h(x) \simeq y\)</span>. Le probl&#232;me que cherche &#224; r&#233;soudre la r&#233;gression lin&#233;aire est de trouver les param&#232;tres <span class="math inline">\(\theta\)</span> qui rendent notre mod&#232;le proche de la r&#233;alit&#233; (repr&#233;sent&#233;e par <span class="math inline">\(y\)</span>) afin qu'il soit efficace.</p>
<p>Si l'on reprend notre &#233;nonc&#233; de d&#233;part sur le prix d'un ordinateur, on a uniquement un attribut (sa puissance), et on cherche une estimation du prix &#224; l'aide d'une fonction d'hypoth&#232;se de la forme :</p>
<p><span class="math inline">\(h_{\theta}(x) = \theta_{0} + \theta_{1}x_1\)</span></p>
<p>Mais pour comprendre comment trouver un bon mod&#232;le, il faut tout d'abord comprendre comment d&#233;crire l'efficacit&#233; d'un mod&#232;le quelconque, et surtout qu'est-ce qui rend un mod&#232;le meilleur qu'un autre ?</p>
<h2 id="fonction-derreur">Fonction d'erreur</h2>
<p>Afin de diff&#233;rencier deux mod&#232;les en fonction de leurs efficacit&#233;s, on va utiliser une <strong>fonction d'erreur</strong> qui mesure le taux d'erreur entre notre mod&#232;le et la r&#233;alit&#233;.</p>
<p>Si on prend un exemple <span class="math inline">\(i\)</span>, la diff&#233;rence entre l'estimation de notre fonction d'hypoth&#232;se et la sortie fournie en entr&#233;e se note :</p>
<p><span class="math inline">\(h_{\theta}(x_{i}) - y_{i}\)</span></p>
<p>O&#249; <span class="math inline">\(x_{i}\)</span> et <span class="math inline">\(y_{i}\)</span> d&#233;signent le <span class="math inline">\(i\)</span>&#232;me exemple sous la forme d'un couple (entr&#233;e, sortie). Cependant, cette valeur peut &#234;tre n&#233;gative, on va donc la monter au carr&#233; car cela nous permet aussi d'amplifier le r&#233;sultat (s'il y a une grosse diff&#233;rence, alors le carr&#233; produira un r&#233;sultat tr&#232;s &#233;lev&#233; et inversement). Si on avait utilis&#233; une autre fonction pour rendre l'expression positive, comme la <a href="https://en.wikipedia.org/wiki/Absolute_value">valeur absolue</a>, on n'aurait pas eu cette deuxi&#232;me propri&#233;t&#233; int&#233;ressante qui permet de mieux distinguer deux mod&#232;les en fonction de leurs efficacit&#233;s.</p>
<p><span class="math inline">\((h_{\theta}(x_{i}) - y_{i})^2\)</span></p>
<p>Vu qu'il y a <span class="math inline">\(m\)</span> exemples en entr&#233;e, on va faire la moyenne de toutes les diff&#233;rences au carr&#233; pour prendre chaque exemple en compte dans notre fonction d'erreur :</p>
<p><span class="math inline">\(\frac{1}{m} \displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^2\)</span></p>
<p>Si on reprend la g&#233;n&#233;ralisation qu'on a faite &#224; la main, la diff&#233;rence que l'on calcule dans notre expression correspond aux parties vertes sur ce sch&#233;ma :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/exemple_calcul_erreur.png" alt="Exemple de calcul de diff&#233;rence entre estimation et r&#233;alit&#233;" />
<p class="caption">Exemple de calcul de diff&#233;rence entre estimation et r&#233;alit&#233;</p>
</div>
<p>On notera cette fonction <span class="math inline">\(J\)</span>, qu'on appelle aussi l'<strong>estimateur des moindres carr&#233;s</strong> :</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{m} \displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^2\)</span></p>
<p>Par convention, et pour simplifier nos futurs calculs, on divise le r&#233;sultat obtenu par 2 :</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2m} \displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^2\)</span></p>
<p>Cette fonction d'erreur nous permet alors de comparer deux mod&#232;les en fonction des param&#232;tres <span class="math inline">\(\theta\)</span> qu'ils utilisent. Il est d'ailleurs possible de d&#233;montrer que cette fonction est un estimateur optimal sous certaines hypoth&#232;ses gr&#226;ce au <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">th&#233;or&#232;me de Gauss-Markov</a>, c'est pour cela qu'elle est utilis&#233;e quasiment dans tous les cas de r&#233;gression lin&#233;aire/polynomiale.</p>
<p>Gr&#226;ce &#224; cela, on peut enfin d&#233;finir concr&#232;tement ce que signifie &quot;trouver le meilleur mod&#232;le&quot;. Cela revient &#224; trouver des param&#232;tres <span class="math inline">\(\theta\)</span> qui <strong>minimisent</strong> la fonction d'erreur utilis&#233;e.</p>
<p>Si l'on affiche graphiquement la fonction d'erreur pour notre probl&#232;me, on obtient ceci :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/exemple_fonction_erreur.png" alt="Exemple de repr&#233;sentation graphique de la fonction d&#39;erreur" />
<p class="caption">Exemple de repr&#233;sentation graphique de la fonction d'erreur</p>
</div>
<p>Sur ce graphique repr&#233;sentant <span class="math inline">\(J\)</span> en fonction de <span class="math inline">\(\theta_{0}\)</span> et <span class="math inline">\(\theta_{1}\)</span>, on remarque clairement les valeurs de <span class="math inline">\(\theta\)</span> pour lesquelles la fonction d'erreur est minimis&#233;e. Cependant, il va falloir trouver un algorithme qui calcule ces valeurs automatiquement, car on ne pourra pas toujours faire de repr&#233;sentation graphique (lorsqu'on a beaucoup d'attributs en entr&#233;e par exemple).</p>
<h2 id="algorithmes">Algorithmes</h2>
<p>On a r&#233;ussi &#224; d&#233;finir math&#233;matiquement l'objectif de la r&#233;gression lin&#233;aire gr&#226;ce &#224; notre fonction d'erreur. D&#233;sormais il faut donc minimiser cette fonction avec les param&#232;tres <span class="math inline">\(\theta\)</span>.</p>
<p>Deux m&#233;thodes r&#233;pandues s'offrent &#224; nous :</p>
<ul>
<li><a href="/algo/ia/apprentissage_artificiel/regression_lin_poly/algo_gradient.html"><strong>L'algorithme du gradient</strong></a> (<em>gradient descent</em> en anglais) : un algorithme it&#233;ratif utile quand <span class="math inline">\(n\)</span> est tr&#232;s large, et personnalisable gr&#226;ce &#224; un coefficient d'apprentissage (ce dernier peut aussi &#234;tre un d&#233;savantage car dans certains cas il est difficile de le choisir efficacement).</li>
<li><a href="/algo/ia/apprentissage_artificiel/regression_lin_poly/equation_normale.html"><strong>L'&#233;quation normale</strong></a> : une &#233;quation donnant le r&#233;sultat directement sans it&#233;rations, cependant cette derni&#232;re est tr&#232;s lourde en op&#233;rations &#224; cause du <a href="https://en.wikipedia.org/wiki/Matrix_multiplication">produit matriciel</a> qui a une complexit&#233; en temps de <span class="math inline">\(O(n^3)\)</span>. On l'utilisera plut&#244;t quand <span class="math inline">\(n\)</span> est suffisamment petit (en g&#233;n&#233;ral en dessous de 10000).</li>
</ul>
<h2 id="r&#233;gression-polynomiale">R&#233;gression polynomiale</h2>
<p>Maintenant qu'on a vu comment fonctionne la r&#233;gression lin&#233;aire, il est temps d'utiliser des fonctions polynomiales plus complexes afin de g&#233;n&#233;raliser sur des donn&#233;es non lin&#233;aires. En r&#233;alit&#233;, l'unique changement &#224; r&#233;aliser est sur notre fonction d'hypoth&#232;se puisque la fonction d'erreur et les deux algorithmes restent exactement les m&#234;mes. Il suffit donc d'employer une <strong>fonction d'hypoth&#232;se polynomiale</strong> :</p>
<p><span class="math inline">\(h_{\theta}(x) = \theta_{0} + \theta_{1}x_1 + \theta_{2}x_2^2 + \ldots + \theta_{n}x_n^d\)</span></p>
<p>Dans cette expression, <span class="math inline">\(d\)</span> correspond au degr&#233; maximum de notre fonction.</p>
<p>Dans le cas o&#249; on a peu d'attributs, et qu'on veut une fonction tr&#232;s complexe, il est tout &#224; fait possible d'utiliser plusieurs fois les m&#234;mes attributs mais avec diff&#233;rents degr&#233;s, par exemple :</p>
<p><span class="math inline">\(h_{\theta}(x) = \theta_{0} + \theta_{1}x_1 + \theta_{2}x_1^2 + \theta_{3}x_1^3\)</span></p>
<p>Il est aussi courant d'ajouter d'autres termes que de simples puissances, comme des exponentiations, des logarithmes, des racines carr&#233;es, des fonctions trigonom&#233;triques, etc. dans le but de mod&#233;liser des fonctions avec un aspect particulier pour bien coller &#224; nos donn&#233;es.</p>
<p>Si possible, afficher les donn&#233;es sur un graphique est la meilleure chose &#224; faire afin de pouvoir visualiser quels types d'attributs il nous faut pour notre fonction d'hypoth&#232;se. Sinon, il est toujours envisageable de tester plusieurs combinaisons et de voir laquelle est la meilleure en fonction du r&#233;sultat de la fonction d'erreur.</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/exemple_regression_polynomiale.png" alt="Exemple de r&#233;gression polynomiale sur des donn&#233;es non-lin&#233;aires" />
<p class="caption">Exemple de r&#233;gression polynomiale sur des donn&#233;es non-lin&#233;aires</p>
</div>
<h2 id="probl&#232;mes">Probl&#232;mes</h2>
<p>Dans le domaine de l'apprentissage artificiel, il y a un probl&#232;me commun &#224; de tr&#232;s nombreux algorithmes : le <strong>surapprentissage</strong>.</p>
<p>Dans un apprentissage supervis&#233;, le but est de fournir &#224; notre algorithme des exemples &#224; partir desquels il peut g&#233;n&#233;raliser le probl&#232;me &#224; r&#233;soudre. Cependant, il arrive que ce dernier ne g&#233;n&#233;ralise pas assez, et en vient &#224; r&#233;citer par c&#339;ur les donn&#233;es fournies. Le probl&#232;me est que notre programme va alors trouver la bonne r&#233;ponse sur quasiment tous nos exemples, mais d&#232;s qu'il verra une nouvelle entr&#233;e il r&#233;pondra totalement &#224; c&#244;t&#233;. Il n'a pas r&#233;ussi &#224; g&#233;n&#233;raliser, et il est tomb&#233; dans le cas par cas. Cette notion de surapprentissage (ou <em>overfitting</em> en anglais) est essentielle &#224; comprendre car c'est un probl&#232;me extr&#234;mement r&#233;current dans le domaine de l'apprentissage artificiel, et sp&#233;cialement dans le cadre d'un apprentissage supervis&#233;.</p>
<p>Par exemple, prenons des donn&#233;es imaginaires :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/exemple_donnees_vide.png" alt="Exemple de donn&#233;es" />
<p class="caption">Exemple de donn&#233;es</p>
</div>
<p>On pourrait tenter d'utiliser une r&#233;gression lin&#233;aire avec une fonction d'hypoth&#232;se de la forme <span class="math inline">\(h_{\theta}(x) = \theta_0 + \theta_1x_1\)</span> :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/exemple_sousapprentissage.png" alt="Tentative de r&#233;gression lin&#233;aire" />
<p class="caption">Tentative de r&#233;gression lin&#233;aire</p>
</div>
<p>On voit bien qu'on arrive &#224; une tr&#232;s mauvaise g&#233;n&#233;ralisation car il nous manque des attributs. Dans ce cas, on parle de <strong>sous-apprentissage</strong> (ou <em>underfitting</em> en anglais), c'est une situation plus rare que le surapprentissage, et il suffit de rajouter des attributs pour contrer le probl&#232;me. Essayons, avec une simple fonction polynomiale comme <span class="math inline">\(h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_1^2\)</span> :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/exemple_regression_polynomiale.png" alt="Une simple fonction polynomiale" />
<p class="caption">Une simple fonction polynomiale</p>
</div>
<p>Notre mod&#232;le polynomial correspond bien &#224; nos donn&#233;es et semble assez bien g&#233;n&#233;raliser le probl&#232;me. Cependant, que se passe-t-il si on avait rajout&#233; plus d'attributs ? Essayons avec une fonction polynomiale plus complexe comme <span class="math inline">\(h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_1^2 + \theta_3x_1^3 + \theta_4x_1^4 \ldots\)</span> :</p>
<div class="figure">
<img src="//static.napnac.ga/img/algo/ia/apprentissage_artificiel/regression_lin_poly/exemple_surapprentissage.png" alt="Fonction polynomiale tr&#232;s complexe" />
<p class="caption">Fonction polynomiale tr&#232;s complexe</p>
</div>
<p>Le mod&#232;le essaie de coller au mieux &#224; nos donn&#233;es au point de ne plus du tout g&#233;n&#233;raliser le probl&#232;me, on est tomb&#233; dans le surapprentissage. Notre programme s'est trop bien adapt&#233; &#224; nos donn&#233;es, et il n'arrivera pas &#224; pr&#233;dire correctement la sortie de nouveaux exemples.</p>
<p>L'apprentissage supervis&#233; est donc un domaine difficile car il faut arriver &#224; trouver les attributs vraiment n&#233;cessaires &#224; notre algorithme pour &#234;tre le plus efficace possible, sans pour autant tomber dans le surapprentissage. Mais il existe des m&#233;thodes afin d'&#233;viter au plus ce probl&#232;me si contraignant, comme la <strong>r&#233;gularisation</strong>.</p>
<h3 id="r&#233;gularisation">R&#233;gularisation</h3>
<p>Le principe de la r&#233;gularisation est de <strong>p&#233;naliser</strong> les attributs avec des coefficients ayant des degr&#233;s &#233;lev&#233;s (puisque c'est &#224; cause d'eux que notre mod&#232;le a une forme tr&#232;s particuli&#232;re qui ne g&#233;n&#233;ralise pas assez). Si l'on reprend notre dernier exemple avec une fonction d'hypoth&#232;se de la forme :</p>
<p><span class="math inline">\(h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_1^2 + \theta_3x_1^3 + \theta_4x_1^4 \ldots\)</span></p>
<p>P&#233;naliser <span class="math inline">\(\theta_3\)</span> et <span class="math inline">\(\theta_4\)</span> permettrait d'avoir un mod&#232;le qui g&#233;n&#233;ralise beaucoup mieux, sans passer par des formes extr&#234;mes.</p>
<p>Pour r&#233;aliser cela, il faut ajouter &#224; notre fonction d'erreur un terme de r&#233;gularisation :</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2m} \left[\displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^2 + \lambda \displaystyle\sum_{j=1}^{n} \theta_j^2\right]\)</span></p>
<p>Dans le terme ajout&#233; <span class="math inline">\(\lambda \displaystyle\sum_{j=1}^{n} \theta_j^2\)</span>, on a <span class="math inline">\(\lambda\)</span> qui correspond au <strong>param&#232;tre de la r&#233;gularisation</strong> (et donc qui d&#233;termine la puissance de la p&#233;nalisation). Il faut aussi noter qu'on ne p&#233;nalise pas <span class="math inline">\(\theta_0\)</span>.</p>
<p>Gr&#226;ce &#224; cela, les coefficients avec des degr&#233;s &#233;lev&#233;s augmenteront fortement le r&#233;sultat de la fonction d'erreur, obligeant naturellement &#224; nos algorithmes de p&#233;naliser ces derniers. On arrive donc &#224; une fonction d'hypoth&#232;se simplifi&#233;e, et moins sujet au cas de surapprentissage.</p>
<p>Cependant il faut adapter nos deux algorithmes &#224; cette nouvelle fonction d'erreur, en les modifiant l&#233;g&#232;rement.</p>
<h4 id="algorithme-du-gradient">Algorithme du gradient</h4>
<p>Avec notre ancienne fonction d'erreur, on devait mettre &#224; jour nos coefficients de mani&#232;re simultan&#233;e de cette fa&#231;on (si l'on n'utilise pas la version vectoris&#233;e) :</p>
<p>Pour chaque coefficient <span class="math inline">\(\theta_j\)</span> avec <span class="math inline">\(j\)</span> allant de 0 &#224; <span class="math inline">\(n\)</span> :</p>
<p><span class="math inline">\(\theta_{j} = \theta_{j} - \alpha\frac{1}{m}\displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})x_{ij}\)</span></p>
<p>D&#233;sormais, on va avoir :</p>
<p>Pour chaque coefficient <span class="math inline">\(\theta_j\)</span> avec <span class="math inline">\(j\)</span> allant de 1 &#224; <span class="math inline">\(n\)</span> (puisqu'on ne p&#233;nalise pas <span class="math inline">\(\theta_0\)</span>, et on utilisera l'ancienne formule pour ce coefficient) :</p>
<p><span class="math inline">\(\theta_{j} = \theta_{j} - \alpha\left[\frac{1}{m}\displaystyle\sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})x_{ij} + \frac{\lambda}{m}\theta_j\right]\)</span></p>
<p>On a obtenu cette formule de la m&#234;me mani&#232;re que pour l'ancienne, c'est-&#224;-dire en calculant la d&#233;riv&#233;e partielle de la fonction d'erreur.</p>
<h4 id="equation-normale">Equation normale</h4>
<p>Pour l'&#233;quation normale, on applique de nouveau notre d&#233;monstration mais sur notre nouvelle fonction d'erreur, ce qui nous donne le r&#233;sultat suivant :</p>
<p><span class="math inline">\(\theta = \left(x^\intercal x + \lambda \begin{bmatrix} 0\\ &amp;1\\ &amp;&amp;1 \\ &amp;&amp;&amp;\ddots \\ &amp;&amp;&amp;&amp;1 \end{bmatrix}\right)^{-1} x^\intercal y\)</span></p>
<p>On retrouve notre param&#232;tre de r&#233;gularisation <span class="math inline">\(\lambda\)</span>, ainsi qu'une matrice de taille <span class="math inline">\((n + 1)\times(n + 1)\)</span> assez sp&#233;ciale compos&#233;e de 1 uniquement dans la diagonale en partant de la deuxi&#232;me colonne (le reste de la matrice contient des 0). Cette matrice est en r&#233;alit&#233; une <a href="https://en.wikipedia.org/wiki/Identity_matrix">matrice identit&#233;</a> sans le premier terme en haut &#224; gauche (en rapport avec <span class="math inline">\(\theta_0\)</span> qui n'est pas p&#233;nalis&#233;).</p>
<h4 id="param&#232;tre-de-r&#233;gularisation">Param&#232;tre de r&#233;gularisation</h4>
<p>Avec un param&#232;tre <span class="math inline">\(\lambda\)</span> tr&#232;s large, on tombe dans le cas du sous-apprentissage car nos coefficients seront tellement p&#233;nalis&#233;s qu'on risque d'avoir une fonction d'hypoth&#232;se trop simple pour notre probl&#232;me. A l'inverse, un param&#232;tre <span class="math inline">\(\lambda\)</span> trop petit ne va pas assez p&#233;naliser les coefficients avec des degr&#233;s &#233;lev&#233;s ce qui n'att&#233;nuera pas notre probl&#232;me de surapprentissage et sera donc inutile.</p>
<p>Il faut alors r&#233;ussir &#224; choisir un bon param&#232;tre de r&#233;gularisation <span class="math inline">\(\lambda\)</span>, et pour cela on peut s'aider de diff&#233;rents &#233;chantillons, ainsi que de la <strong>validation crois&#233;e</strong>. Jusqu'&#224; pr&#233;sent, le seul &#233;chantillon de nos donn&#233;es qu'on utilisait &#233;tait <strong>l'&#233;chantillon d'apprentissage</strong>. On va d&#233;sormais rajouter deux nouveaux &#233;chantillons :</p>
<ul>
<li><strong>l'&#233;chantillon de test</strong> : on l'utilisera pour mesurer l'efficacit&#233; de notre algorithme sur de nouvelles donn&#233;es, car si on mesure cela sur notre &#233;chantillon d'apprentissage et que notre algorithme a un probl&#232;me de surapprentissage, on verra de tr&#232;s bons r&#233;sultats alors qu'on a un programme m&#233;diocre incapable de g&#233;n&#233;raliser.</li>
<li><strong>l'&#233;chantillon de validation</strong> : on va employer cet &#233;chantillon afin de tester diff&#233;rentes valeurs de <span class="math inline">\(\lambda\)</span> et s&#233;lectionner la meilleure.</li>
</ul>
<p>On n'utilisera pas l'&#233;chantillon de test dans le choix du param&#232;tre <span class="math inline">\(\lambda\)</span>, mais il est important d'en parler car en g&#233;n&#233;ral sur nos donn&#233;es on les divise entre nos diff&#233;rents &#233;chantillons de tel sorte &#224; avoir environ 60% des donn&#233;es dans l'&#233;chantillon d'apprentissage, 20% dans celui de test, et 20% dans celui de validation.</p>
<p>Le principe de la validation crois&#233;e est de tester diff&#233;rentes valeurs de <span class="math inline">\(\lambda\)</span> et s&#233;lectionner la meilleure gr&#226;ce &#224; notre fonction d'erreur et &#224; nos &#233;chantillons :</p>
<ul>
<li>G&#233;n&#233;rer diff&#233;rents param&#232;tres de r&#233;gularisation (0, 0.01, 0.02, 0.04, ..., 1, ... 10, ...).</li>
<li>Pour chaque param&#232;tre <span class="math inline">\(\lambda\)</span> &#224; tester, calculer les coefficients <span class="math inline">\(\theta\)</span> en minimisant <span class="math inline">\(J\)</span> (version r&#233;gularis&#233;e).</li>
<li>Pour chaque coefficient obtenu, calculer le taux d'erreur par rapport &#224; notre &#233;chantillon de validation (encore inconnue du programme) en utilisant la fonction d'erreur non r&#233;gularis&#233;e sur cet &#233;chantillon : <span class="math inline">\(J_{validation}\)</span> (on n'utilise pas l'&#233;chantillon de test car il ne faut pas que notre algorithme voit les donn&#233;es de cet &#233;chantillon avant d'&#234;tre totalement entrain&#233;).</li>
<li>Choisir <span class="math inline">\(\lambda\)</span> qui obtient le plus faible taux d'erreur sur la derni&#232;re &#233;tape.</li>
</ul>
<p>Notez qu'on peut utiliser cette m&#233;thode de validation crois&#233;e afin de choisir les degr&#233;s &#224; utiliser dans notre fonction d'hypoth&#232;se polynomiale de la m&#234;me fa&#231;on que pour <span class="math inline">\(\lambda\)</span>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>La r&#233;gression lin&#233;aire/polynomiale est donc un moyen de g&#233;n&#233;raliser un probl&#232;me &#224; partir d'exemples fournis en construisant un mod&#232;le plus ou moins complexe. On a pu voir deux algorithmes tr&#232;s diff&#233;rents, ainsi que le principal probl&#232;me li&#233; &#224; ce type d'apprentissage avant d'aborder une solution efficace.</p>
<p>M&#234;me si l'action de &quot;g&#233;n&#233;raliser&quot; est une notion assez facile &#224; appr&#233;hender en tant qu'humain, c'est bien plus compliqu&#233; de le faire comprendre &#224; un ordinateur et les math&#233;matiques nous permettent de nous en rapprocher consid&#233;rablement comme on a pu le voir. L'algorithme du gradient sera d'ailleurs utilis&#233; &#224; travers d'autres algorithmes d'apprentissage artificiel, il &#233;tait donc important de le d&#233;couvrir ici dans un cadre assez accessible.</p>
<p>D&#233;duire un mod&#232;le &#224; partir de donn&#233;es est un probl&#232;me tr&#232;s commun, et la r&#233;gression lin&#233;aire/polynomiale est une m&#233;thode employ&#233;e dans pleins de domaines comme l'&#233;conomie, la finance, les statistiques, la g&#233;ographie, la physique, la biologie, etc.</p>
 
      <!-- ------------ -->

      <footer>
         <br>
         <hr>
         <p>Une question ? Une suggestion ? N'h&#233;sitez pas &#224; me <a href="/a_propos.html">contacter</a> pour me communiquer vos remarques.
         <br>
      </footer>

      <!-- Automatically render all of the math inside the page with KaTeX -->
      <script>
         renderMathInElement(document.body);
      </script>

   </body>
</html>
